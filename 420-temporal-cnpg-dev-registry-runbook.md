# 420 – Temporal CNPG creds + dev registry rollout (runbook and incident notes)

> **Cross-References (Deployment Architecture Suite)**:
>
> | Document | Purpose |
> |----------|---------|
> | [465-applicationset-architecture.md](465-applicationset-architecture.md) | How data-temporal apps are generated |
> | [447-waves-v3-cluster-scoped-operators.md](447-waves-v3-cluster-scoped-operators.md) | Rollout phases: 265 (migrations), 450/455 (runtime/bootstrap) |
> | [464-chart-folder-alignment.md](464-chart-folder-alignment.md) | Chart at `sources/charts/third_party/temporal/` |
> | [426-keycloak-config-map.md](426-keycloak-config-map.md) | CNPG credential ownership pattern |
>
> **Related**:
> - [423-temporal-argocd-recovery.md](423-temporal-argocd-recovery.md) – ArgoCD recovery procedures
> - [425-vendor-schema-ownership.md](425-vendor-schema-ownership.md) – Vendor schema patterns
> - [429-devcontainer-bootstrap.md](429-devcontainer-bootstrap.md) – DevContainer bootstrap
> - [456-ghcr-mirror.md](456-ghcr-mirror.md) – GHCR mirroring for Temporal images

> ⚠️ **Legacy workflow:** References to the k3d dev registry (`k3d-ameide.localhost:5001`) describe the previous local-cluster model. With the remote-first pivot (backlog/435), dev builds push directly to ACR and run on AKS; keep these notes only for historical incident context.
>
> **Bootstrap location:** When this runbook mentions `tools/bootstrap/bootstrap-v2.sh`, substitute the GitOps bootstrap now located at `ameide-gitops/bootstrap/bootstrap.sh`. Developer DevContainers only run `tools/dev/bootstrap-contexts.sh` to point kubectl/Telepresence/argocd at the shared AKS cluster.

## What we configured
- **Temporal DB ownership:** CNPG (`platform-postgres-clusters`) owns the `temporal` and `temporal_visibility` roles/secrets. The Temporal chart now uses `temporal`/`temporal_visibility` users via `temporal-db-credentials` and `temporal-visibility-db-credentials`. All Vault-authored ExternalSecrets for Temporal were removed; Vault is mirror-only if needed. See also `backlog/425-vendor-schema-ownership.md` for the general vendor schema pattern.
- **Schema hook matches upstream flow:** `data-migrations-temporal` runs the same steps as Temporal's `auto-setup.sh`. The Helm hook job:
  - Uses the GHCR-mirrored `ghcr.io/ameideio/mirror/temporalio-admin-tools` image (see `backlog/456-ghcr-mirror.md`) and runs the bundled `temporal-sql-tool` (version pinned via `values.schema.toolVersion`, kept in lockstep with the Temporal server image tag) instead of downloading it at runtime, matching Temporal's self-hosted guidance.
  - Always executes `setup-schema -v 0.0` for both databases (`postgresql/v12/temporal` and `/visibility`). This is idempotent and initializes `schema_version` if missing.
  - Immediately follows with `update-schema --schema-name ...` so the database is migrated to the latest manifest bundled with the chart. There is no manual `TargetVersion`; whatever is in `postgresql/v12/**/versioned` is what gets applied.
  - Uses only the CNPG-provided secrets (`temporal-db-env`, `temporal-visibility-db-env`) for connection info; no Flyway secrets or additional RBAC required.
  - Runs as a Helm hook (`pre-install,pre-upgrade`, weight -9) so it completes before the Temporal Deployment rolls out. Retrying the Argo app simply reruns the same setup/update sequence safely.
- **Env overlays aligned:** Local/staging/production overlays now also reference the CNPG app secrets (`temporal-db-credentials`, `temporal-visibility-db-credentials`) instead of the admin `postgres-ameide-auth` secret, so every environment uses the same application-scoped creds.
- **Dev registry wiring:** k3d local registry `k3d-ameide.localhost:5001/ameide` is the single endpoint for dev GitOps. Builds push via the host-published port `localhost:5001` (configurable via `AMEIDE_REGISTRY_PUSH_HOST`) to avoid DNS-to-container IP hangs.
- **Image naming:** Build script enforces hyphenated image tags (`agents-runtime`, `www-ameide`, `www-ameide-platform`, etc.) even if service directories use underscores. Filters accept either form.
- **Argo CD apps:** `data-temporal` and `data-temporal-namespace-bootstrap` are generated by the `ameide-dev` `ApplicationSet` (`gitops/ameide-gitops/environments/dev/argocd/apps/ameide.yaml`). The template now:
  - Defaults `syncPolicy.automated` to `prune: true`, `selfHeal: true` for apps that don’t opt out.
  - Applies a shared `retry` policy (`limit: 10` with exponential backoff up to 5m) so transient bootstrap failures (e.g., ExternalSecrets webhook, CNPG readiness) are retried automatically.
  - Uses RollingSync `rolloutPhase` ordering so `data-migrations-temporal` (migration) runs before `data-temporal` (runtime) and `data-temporal-namespace-bootstrap` (namespace job).
- **Temporal migrations sync policy:** The `data-migrations-temporal` Application now keeps `automated` sync but sets `prune: false` and `selfHeal: false`. The Helm hook still runs automatically on first install or whenever we intentionally sync/upgrade, but Argo no longer keeps re-creating the job after it deletes itself. This prevents repeated reruns of `setup-schema` that crash with duplicate key errors.

In dev, the **normal path** is now: open the DevContainer, let the GitOps bootstrap (`ameide-gitops/bootstrap/bootstrap.sh`) converge the cluster (see `backlog/429-devcontainer-bootstrap.md` for legacy context), and rely on Argo’s automated sync + retry + RollingSync to bring Temporal apps healthy. The CLI `argocd app sync` steps below remain the manual recovery path when something is stuck (see also `backlog/423-temporal-argocd-recovery.md`).

## Issues we hit and fixes
- **Temporal CrashLoopBackOff (permission denied on `schema_version`):**
  - **Cause:** Temporal pods were using the generic `dbuser`, which lacked grants on `schema_version`; sometimes the schema table was absent after fresh cluster brings.
  - **Fix:** Switched chart values to the dedicated roles (`temporal`, `temporal_visibility`), removed Vault-driven secrets, and relied on CNPG-managed secrets. Ensured the Temporal schema hook (`data-migrations-temporal`) and namespace bootstrap run after sync (manual `argocd app sync data-migrations-temporal data-temporal{,-namespace-bootstrap}`).
- **Temporal CrashLoopBackOff (relation `schema_version` does not exist):**
  - **Cause:** Fresh cluster without Temporal schemas initialized (Temporal migrations/bootstrap not yet applied).
  - **Fix:** Sync `data-migrations-temporal`, then `data-temporal` and `data-temporal-namespace-bootstrap`; confirm `schema_version` exists via `psql` against CNPG. Post-sync, all Temporal deployments went Running.
- **Dev registry push hangs / missing images:**
  - **Cause:** Devcontainer resolving `k3d-ameide.localhost` to the registry container IP (no host port), and underscore image names diverging from GitOps values.
  - **Fix:** Build script now defaults push endpoint to `localhost:5001` while tagging `k3d-ameide.localhost:5001/ameide/<image>:dev`; enforces hyphenated tags; adds service filters; documents usage in README/backlog/415.
- **Temporal schema job reran endlessly (duplicate `namespace_metadata` PK):**
  - **Cause:** Because Argo self-healed deleted hook resources, the Helm hook job re-created itself after every sync. After the first successful `setup-schema`, the second rerun hit `duplicate key value violates unique constraint "namespace_metadata_pkey"` and exited before `update-schema` could bump the version, leaving Temporal stuck at `0.0`.
  - **Fix:** Disabled `prune`/`selfHeal` for the `data-migrations-temporal` Application so the hook only runs when we explicitly sync or bump the chart. Added the manual cleanup procedure below to clear any partially-initialized tables before the next intentional sync.

## Reproducible rollout (dev)
1) Pull repo + submodule: `git pull --recurse-submodules && git submodule update --init --recursive`.
2) Bootstrap dev cluster: `ameide-gitops/bootstrap/bootstrap.sh --config bootstrap/configs/dev.yaml` (builds + pushes dev images to the designated registry).
3) Port-forward and log into ArgoCD:
   ```
   kubectl -n argocd port-forward svc/argocd-server 8080:80 >/tmp/argocd-pf.log 2>&1 &
   ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d)
   argocd login localhost:8080 --username admin --password "$ARGOCD_PASSWORD" --insecure --grpc-web
   ```
4) If Temporal remains degraded after bootstrap, or you want to force a clean recovery, sync Temporal schema + apps and wait for health:
   ```
   argocd app sync data-migrations-temporal --grpc-web --server localhost:8080
   argocd app sync data-temporal --grpc-web --server localhost:8080
   argocd app sync data-temporal-namespace-bootstrap --grpc-web --server localhost:8080
   argocd app wait data-temporal data-temporal-namespace-bootstrap --health --timeout 180 --grpc-web --server localhost:8080
   ```
5) Verify pods: `kubectl get pods -n ameide | grep temporal` (all Running; namespace bootstrap Completed). DB check (optional): `psql -U temporal -d temporal -c "select * from schema_version;"` on a CNPG pod.

### Manual cleanup when `namespace_metadata_pkey` is duplicated

If `data-migrations-temporal-temporal-migrations` fails with:

```
duplicate key value violates unique constraint "namespace_metadata_pkey"
```

the schema setup phase re-ran after partially initializing the default namespace. Clean up and rerun the hook once:

1. Delete the stuck hook job so we start from a clean slate:
   ```
   kubectl -n ameide delete job data-migrations-temporal-temporal-migrations
   ```
2. Remove the duplicate namespace row from CNPG (pick the primary pod for `platform-postgres-clusters`):
   ```
   CNPG=$(kubectl -n ameide get pods -l cnpg.io/cluster=platform-postgres-clusters,role=primary -o jsonpath='{.items[0].metadata.name}')
   kubectl -n ameide exec -it "${CNPG}" -- psql -U temporal -d temporal -c \
     "delete from namespace_metadata where namespace_id='namespace|default';"
   ```
3. Trigger the hook exactly once:
   ```
   argocd app sync data-migrations-temporal --grpc-web --server localhost:8080
   ```
   Wait for the hook pod to finish and confirm `temporal-sql-tool version` now matches the chart’s expected version (currently `1.18`).
4. Re-sync the runtime and namespace bootstrap apps as in step 4 above.

## Follow-ups / hardening
- Auto-sync + retry for `data-temporal*` in dev is now configured via the shared ApplicationSet template. Manual `argocd app sync`/`app wait` is reserved for explicit recovery (especially after schema or CNPG incidents).
- Add a guardrail smoke to fail fast if `schema_version` is missing or if Temporal roles lack table privileges (e.g., simple readiness check pod).
- Keep dev Tilt registry config separate; do not point Tilt at `k3d-ameide.localhost` unless intentionally sharing artifacts with Argo.

For related design and runbooks, see:

- `backlog/425-vendor-schema-ownership.md` – vendor vs Flyway schema ownership (Temporal pattern).
- `backlog/423-temporal-argocd-recovery.md` – detailed recovery flows when Temporal or namespace bootstrap fail in Argo.
- `backlog/429-devcontainer-bootstrap.md` – how the DevContainer + `bootstrap-v2.sh` orchestrate k3d, Argo, and GitOps end-to-end.
- `backlog/456-ghcr-mirror.md` – GHCR mirroring for Docker Hub rate limiting (Temporal uses GHCR mirrors for server, admin-tools, and UI images).
