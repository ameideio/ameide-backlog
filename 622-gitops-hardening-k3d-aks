---
title: "622 – GitOps Hardening: k3d (local) + AKS (dev) parity"
status: draft
owners:
  - platform-sre
  - gitops
---

# 622 – GitOps Hardening: k3d (local) + AKS (dev) parity

## Problem statement

We must be able to iterate on **AKS dev** without breaking **local k3d**, and vice-versa.
Every “fix” must be evaluated across **both targets** and must result in a configuration that is:

- **Vendor-aligned** (Kubernetes + Azure AKS + OCI image supply chain).
- **Deterministic** (no “works if you click it twice”; no silent fallbacks).
- **Scoped** (cluster-scoped resources are owned once; env-scoped resources don’t fight each other).

This backlog captures the current failure modes we observed and defines a **single, convergent approach** for both local and dev.

## Scope

### In scope

- GitOps configuration parity for **local k3d** and **AKS dev**.
- Cluster bootstrap and ArgoCD reconciliation hardening (render validity, image policy, scheduling).
- Operator + data-plane correctness for both targets (Temporal operator/webhooks, Strimzi/Kafka scheduling, Gateway API routes).
- CI guardrails that prevent “fix dev, break local”.

### Out of scope

- Re-architecting the entire app portfolio; we focus on minimum changes that restore full health and prevent regressions.
- Terraform/provider changes unrelated to GitOps parity (tracked under `backlog/444-terraform-v2.md`).

## Target state (definition of done)

1. **Both targets converge** from a clean bootstrap:
   - local: `k3d` + local terraform + GitOps bootstrap results in “baseline apps” green.
   - dev: GitHub CI apply/verify results in “baseline apps” green.
2. **No Argo ComparisonError** for applications that are declared “installed”.
3. **No SharedResourceWarning** (cluster-scoped resource ownership is unambiguous).
4. **No hostPort collisions** in multi-env AKS (dev/staging/prod share the same worker nodes).
5. **Images are multi-arch** (linux/amd64 + linux/arm64) wherever local needs arm64 and AKS is amd64.
6. CI includes a **render + conformance gate** for both local and dev overlays.

## Current findings (AKS dev) and how they relate to local

### A) Docker Hub pull rate limiting blocked reconciliation (AKS) and was masked locally

**Observed in AKS dev:** many pods failed with `429 Too Many Requests` from `docker.io`, leaving multiple Argo apps `Progressing/Degraded`.

**Why local didn’t show it consistently:** local k3d defaults to `imagePullPolicy: IfNotPresent` (see `sources/values/cluster/local/globals.yaml`) and tends to have warmer caches; AKS nodes were repeatedly pulling from Docker Hub.

**Remediation (parity-safe, vendor-aligned):**
- Make `docker.io` a “do not depend on at runtime” registry for shared workloads.
- Use:
  - public mirrors for “docker library” images (see `backlog/456-ghcr-mirror.md` + shared values),
  - a GHCR mirror for third-party images that don’t have a stable non-DockerHub source,
  - CI-driven mirroring so the cluster never needs Docker Hub (see `.github/workflows/mirror-images.yaml`).

**Hard rule (avoid band-aids):** fix image sources in `sources/values/_shared/**` and `sources/values/env/**` (the ApplicationSet layering inputs). Do not “fix” in `sources/charts/shared-values/**` unless a chart explicitly consumes it.

### B) Multi-arch image integrity (amd64 + arm64) is a hard requirement

**Observed in AKS dev:** images pinned to a single-arch digest caused `no match for platform in manifest` on amd64 nodes (or `exec format error` when the wrong arch image was used).

**Why this is a parity issue:** k3d (developer laptops) is often `linux/arm64`, AKS is `linux/amd64`. Any shared image must be a **manifest list** digest, not a single-arch digest.

**Remediation:**
- For smoke jobs: replace the broken `grpcurl-runner` digest with a multi-arch upstream (`fullstorydev/grpcurl`) mirrored to GHCR and pinned by digest.
- For app images: only pin digests that are known multi-arch (see `backlog/603-image-pull-policy.md`).
- For ARC: ensure the runner image is published as a multi-arch manifest list and GitOps pins the manifest digest (not an arch-specific digest).

### C) Cluster-scoped collisions across envs (SharedResourceWarning)

**Observed:** Kubernetes Dashboard installs per env can collide on cluster-scoped objects (`metrics-scraper` ClusterRole/Binding), producing `SharedResourceWarning` and preventing clean convergence.

**Remediation:**
- Keep the Dashboard env-scoped, but scope any cluster-scoped names per env.
- Concrete implementation: set `metricsScraper.role` uniquely per env in:
  - `sources/values/env/dev/platform/kubernetes-dashboard.yaml`
  - `sources/values/env/staging/platform/kubernetes-dashboard.yaml`
  - `sources/values/env/production/platform/kubernetes-dashboard.yaml`
  - `sources/values/env/local/platform/kubernetes-dashboard.yaml`
  while keeping shared defaults in `sources/values/_shared/platform/kubernetes-dashboard.yaml`.

### D) Secrets and pull credentials must be namespace-complete (Vault + ExternalSecrets)

**Observed:** `clickhouse-system` had `ExternalSecret` resources that referenced `SecretStore/ameide-vault`, but the SecretStore didn’t exist in that namespace. Result: `ghcr-pull` never materialized there and private image pulls failed (401).

**Related backlogs:**
- `backlog/446-namespace-isolation.md` (SecretStore is namespace-scoped)
- `backlog/452-vault-rbac-isolation.md` (cross-namespace consumers must be explicitly declared)
- `backlog/419-clickhouse-argocd-resiliency.md` (operator namespace + resiliency)

**Remediation:**
- Extend `extraSecretStores` to include `clickhouse-system` in:
  - `sources/values/env/production/foundation/foundation-vault-secret-store.yaml`
  - `sources/values/env/local/foundation/foundation-vault-secret-store.yaml`

### E) Local must not require the cloud “runtime-facts repo” to render

**Observed (local k3d):** Argo CD repo-server ComparisonErrors when it could not fetch `https://github.com/ameideio/ameide-runtime-facts.git`.

**Why this breaks parity:** local bootstrap should be self-contained (devs should not need access to a separate CI-owned repo to get a working local cluster).

**Remediation (no drift):**
- Patch the local overlay to source “runtime facts” from this repo for local only:
  - `argocd/overlays/local/kustomization.yaml`
- Provide an empty placeholder contract so Helm valueFiles always resolve:
  - `runtime-facts/cluster/local/globals.yaml`
  - `runtime-facts/env/local/globals.yaml`

### F) Remaining “full health” work (must be solved without breaking local)

These are still present in AKS and must be addressed with changes that also make sense for local:

- Kafka `Progressing`: remove env-specific pool/affinity assumptions and converge on a provider-neutral node profile (see `backlog/421-argocd-strimzi-kafkanodepool-health.md` + `backlog/444-terraform-v2.md`).
- Prometheus `Progressing`: avoid per-env hostPort DaemonSets in multi-env clusters; deploy node-exporter once at cluster scope (see `backlog/447-third-party-chart-tolerations.md`).
- `dev-process-transformation-v0-ingress` CrashLoopBackOff: treat as an application-level failure (needs logs + config verification) but must be reproducible locally.
- Gateways `Degraded` in staging/prod due to missing backend Service for GRPCRoutes: decide whether the routes are env-conditional or the backend must exist in those envs.

### G) Guardrails (to prevent recurrence)

- CI must verify:
  - render validity for both local and azure overlays (values layering),
  - no runtime-critical pulls from Docker Hub for shared workloads,
  - multi-arch digest validity for shared images,
  - namespace completeness for SecretStore + ExternalSecrets where cross-namespace secrets are required.

## Cross-target “boundaries” (to prevent spaghetti)

## Allowed target-specific deltas (explicit and intentional)

These deltas are expected and must remain **explicitly scoped to env overlays** (not scattered conditionals):

- **Local** keeps `k3d` + local Terraform for cluster lifecycle and fast iteration.
- **Local** does not use Azure DNS / Let’s Encrypt DNS01; Azure Workload Identity helpers are disabled where appropriate.
- **Local** must remain compatible with **linux/arm64** (common on Apple Silicon), while **AKS dev** is **linux/amd64**.
- **Local** may need “relaxed scheduling” tolerations to allow stateful pods on control-plane nodes.

Everything else should converge (same charts, same health semantics, same required values, same schema validation).

### 1) Cluster-scoped vs env-scoped ownership

**Cluster-scoped (one owner per cluster):**
- CRDs and operators (cert-manager, external-secrets, strimzi, temporal-operator, cnpg, etc.)
- Gateway / shared ingress dataplane and its “global” routes
- Any hostPort DaemonSets (node-exporter)

**Env-scoped (one owner per env namespace):**
- App workloads and env-specific routes
- Env-specific secrets materialization and seed jobs

Rule: env-scoped apps must not create cluster-scoped objects with stable names unless they include env scoping in the name.

### 2) Image supply chain contract (local + dev)

- Any image used by both targets must be published as a **multi-arch manifest list** (amd64 + arm64).
- GitOps pins **manifest list digests** (not single-arch digests) so both runtimes pull the correct platform.
- Local may route images via a **local registry** (k3d) or a **GHCR mirror**, but the pinned reference must remain deterministic (digest pinned; no floating tags).
- CI must detect:
  - missing `:main` channel tags for local/dev automation
  - missing multi-arch manifests
  - missing required `image.ref` values for installed apps

## Implementation notes (2026-01)

### ARC runner parity + routing

- ARC runner sets exist in both targets:
  - Local: `arc-local`
  - AKS: `arc-aks`
- Workflow routing is GitHub-driven only via `vars.AMEIDE_RUNS_ON` (no workflow defaults).
- Runner registration is repo-scoped (`githubConfigUrl: https://github.com/ameideio/ameide-gitops`) to avoid org runner-group allowlist failures.

### BuildKit cross-arch support

- `buildkit/binfmt` installs emulation for `amd64,arm64` so a runner on either arch can build both platforms when needed.

### 3) Scheduling contract (pools and profiles)

We cannot rely on “env pools” if dev is a shared-node cluster.

Target direction:
- **Stateful services** → standard, non-preemptible capacity (future “stateful/standard” profile).
- **Stateless services** → can prefer spot (future “spot” profile) but must safely fallback.
- **Local** has no pools; its profile must be either “none” or “relaxed required + preferred”.

Short-term compatibility:
- Keep `config/node-profiles/none.yaml` for local.
- Use `config/node-profiles/general-pool.yaml` for AKS until the stateful/spot split is introduced.
- Remove remaining env-specific affinity/toleration assumptions from env overlays.

## CI guardrails (must cover both local and dev)

Add a GitHub Actions job (or extend existing gates) that, on every PR:

1. Renders a representative set of Helm charts for:
   - `env/local` with `config/node-profiles/none.yaml`
   - `env/dev` with `config/node-profiles/general-pool.yaml`
2. Validates:
   - no Helm render errors (schema-required values must exist)
   - Kubernetes schema conformance (Gateway API, CRD presence constraints)
   - no duplicate cluster-scoped resource names across envs (SharedResourceWarning patterns)
   - no runtime-critical pulls from `docker.io` for the AKS overlay (allowlist only if needed)
   - multi-arch digest validity for shared images (manifest list, not single-arch)

This is the enforcement mechanism that prevents “fix dev, break local”.

## Seeding status (AKS dev)

Current evidence that baseline seeding/migrations ran:
- `dev-platform-dev-data` is `Synced/Healthy`.
- `job.batch/data-db-migrations` in `ameide-dev` completed successfully (`succeeded=1`).

This aligns with the contract in `backlog/582-local-dev-seeding.md` (same seed/verify approach).

## Work plan (prioritized)

1. **Run CI image mirroring + resync** (Dashboard/Kong + smoke tools + any remaining DockerHub blockers).
2. **Confirm secret plumbing is namespace-complete** (SecretStore + `ghcr-pull` in `clickhouse-system`).
3. **Resolve remaining app-level Degraded states**:
   - `dev-process-transformation-v0-ingress` CrashLoopBackOff (logs + config parity with local)
   - staging/prod Gateway `Degraded` due to missing GRPC backend Service (decide env-conditional route vs deploy backend)
4. **Normalize shared-cluster scheduling**:
   - Kafka scheduling (remove env-specific pool assumptions)
   - Prometheus hostPort DaemonSets (node-exporter strategy)
5. **Add CI guardrails** (render + schema + image policy checks for local+dev).

## Related backlogs / references

- `backlog/444-terraform-v2.md` (CI-owned AKS; remove env-specific pools; long-term standard/spot split)
- `backlog/465-applicationset-architecture-preview-envs-v2.md` (values layering; gateway route requirement)
- `backlog/582-local-dev-seeding.md` (seed/verify contract)
- `backlog/603-image-pull-policy.md` / `backlog/602-image-pull-policy.md` (digest pinning + :main automation)
- `backlog/420-temporal-cnpg-dev-registry-runbook.md` (Temporal operator/webhook and recovery)
- `backlog/447-third-party-chart-tolerations.md` (DaemonSet hostPort conflicts)
- `backlog/456-ghcr-mirror.md` (multi-arch + mirror pitfalls)
- `backlog/612-kubernetes-dashboard.md` (dashboard deployment model)
- `backlog/454-coredns-cluster-scoped.md` (SharedResourceWarning is misconfiguration)
