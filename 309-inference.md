# 309: Inference Service Architecture Upgrade

**Status**: Draft  
**Priority**: High  
**Complexity**: Large

---

## 1. Problem Statement

The Python inference service still behaves like the original threads MVP in critical areas:

- Agent/tool definitions are generated in-process via `iter_agent_configs()` and cached into `AGENT_CONFIGS`, requiring code changes + redeploys for any update instead of pulling from an external catalog.
- Tool factories close over environment-specific credentials, preventing central dependency management.
- Agent routing is implemented as imperative `if/elif` logic, making per-tenant customization impossible.
- The gRPC surface emitted by the service lags behind the gateway: only REST `/agents/stream` is production-grade; LangGraph upgrades require manual wiring.

As a result, operations must still ship code to change prompts or tool bindings, and the inference gateway cannot reason about agent capabilities despite the partial registry.

---

## 2. Objectives

1. **Registry-driven runtime** – Load agent and tool definitions at runtime (from the new Agents service) and compile LangGraph graphs without editing source code.
2. **Declarative routing** – Replace `select_agent_id()` with a rule engine so the inference gateway can evaluate tenant-specific conditions.
3. **DI & tool isolation** – Inject gRPC clients and runtime dependencies explicitly, mirroring n8n’s factory pattern.
4. **Protocol parity** – Expose first-class gRPC `Invoke` and `Stream` APIs with structured event streaming, matching the gateway contract.
5. **LangChain v1 alignment** – Build runtime graphs with `langchain.agents.create_agent`, surface middleware hooks, and forward standard `content_blocks` so modern multimodal LLM features stay available.
6. **Observability & safety** – Tag traces with agent instance data, record tool executions, and enforce iteration/timeout limits pulled from configuration.

## Current Progress (2025-02)

- **✅ Registry scaffolding:** `Registry.refresh_nodes/list_agents` (`services/agents_runtime/src/agents_service/registry.py`) hydrates catalog data via unary `ListNodes`/`ListAgents` calls and exposes tenant instances to the gRPC layer, but it relies on polling—`StreamCatalog` is unused and agent mutations are only observed after the next refresh.
- **✅ gRPC façade:** `Invoke`, `Stream`, and `ListAgents` endpoints are wired through the gateway (`services/agents_runtime/src/agents_service/service.py`), matching the proto contract and routing metadata envelope.
- **✅ LangChain runtime stub:** `LangGraphRuntime` now hydrates `create_agent` graphs when LangChain is installed and falls back to a deterministic local model otherwise (`services/agents_runtime/src/agents_service/runtime.py`). Structured `content` events are emitted alongside status/token/usage, but middleware/tool wiring from artifacts remains TODO.
- **✅ Integration harness:** The agents_runtime image bakes the gRPC integration tests (`__tests__/integration`), and Tilt’s `test:integration-agents_runtime` resource now runs via the shared Helm job—no ad-hoc `pip install` steps remain.
- **⚠️ Artifact fidelity:** Compiler persists a `plan.v1` snapshot (model/prompt/tool ids) (`services/agents/internal/compiler/compiler.go`); compiled LangGraph DAGs and middleware stacks are still missing.
- **⚠️ Artifact cache:** Runtime caches per-agent executors in-process keyed by artifact hash, but there is no shared Redis cache or eviction policy for multi-replica deployments.
- **⚠️ Streaming schema gap:** `inference_service.proto` emits status/token/tool events only; there is no `content`/`content_blocks` channel, so multimodal payloads generated by LangChain v1 would be dropped even after the runtime upgrade.
- **⚠️ Event diffusion:** Catalog broadcasts only push node definitions; agent changes rely on manual `BroadcastCatalogSnapshot`, leaving long-lived subscribers unaware of new instances until the next full refresh.
- **⚠️ Placeholder capabilities:** `ListAgents` responses surface empty `RateLimits`, ignore request filters, and return raw tool IDs (`services/agents_runtime/src/agents_service/service.py`); gateway budgeting and capability introspection still require hard-coded fallbacks.

> **Direction:** Migrate runtime construction to `langchain.agents.create_agent` (LangChain v1), persist the resulting LangGraph graph (with middleware + context schema) as the compiled artifact, and forward `content_blocks` through the gRPC interface. Introduce an artifact cache layer and extend the registry broadcaster to publish agent-instance deltas so the inference service stays warm without polling.

---

## 3. n8n Guidance Applied to Inference

| n8n Pattern | Reference | Inference Translation |
|-------------|-----------|------------------------|
| Versioned node registry | `nodes/agents/Agent/Agent.node.ts` | Cache `NodeDefinition` + `NodeVersion` from Agents service; keep multiple versions loaded simultaneously. |
| Declarative property schema | `Agent/V3/AgentV3.node.ts` | Convert proto `PropertySchema` into LangGraph builder parameters (prompts, streaming flags, fallback models). |
| Reusable option bundles | `agents/ToolsAgent/options.ts` | Map common options (system message, max iterations, return intermediate steps) to LangGraph config struct. |
| Co-located execution helpers | `agents/ToolsAgent/V3/execute.ts` | Implement runtime adapters that create LangGraph agents, handle streaming, and dispatch tool calls. |
| Tool factories | `tools/ToolCalculator/ToolCalculator.node.ts` | Wrap each tool in a DI-friendly factory that accepts shared clients (graph, element, BPMN). |
| Tool executor | `ToolExecutor/ToolExecutor.node.ts` | Provide a generic executor for toolkit-style tools so LangGraph can call them uniformly. |

---

## 4. Target Inference Architecture

```
┌─────────────────────────────┐      ┌──────────────────────────┐
│ Agents Service (catalog)    │─────▶│ Inference Service        │
│  - StreamCatalog gRPC       │      │  - Runtime registries    │
│  - Agent instance configs   │      │  - LangGraph compiler    │
└─────────────────────────────┘      │  - Tool runtime (DI)     │
                                     │  - Routing engine        │
                                     └──────────▲───────────────┘
                                                │
                                     ┌──────────┴───────────┐
                                     │ Inference Gateway    │
                                     │  - Auth, rate limits │
                                     │  - Context envelope  │
                                     │  - Request fan-out   │
                                     └──────────▲───────────┘
                                                │
                                     ┌──────────┴───────────┐
                                     │ Clients / Workflows  │
                                     │  - Web UI (threads)     │
                                     │  - Temporal jobs     │
                                     │  - Internal services │
                                     └──────────────────────┘
```

Supporting components:
- **Dependency pool** – async gRPC clients (graph, element, BPMN) managed via `ToolDependencies`.
- **State store** – Postgres checkpointer + optional Redis for short-term context.
- **Telemetry** – Langfuse handler attached to each graph execution with agent instance tags.

---

## 5. Runtime Design

### 5.1 Execution Modes

- **Direct invocation** – Latency-sensitive clients (threads UI, preview tools) continue to call the inference service via gRPC `Invoke`/`Stream`. These requests hydrate LangGraph artifacts, run middleware, and stream tokens exactly as described in this document.
- **Temporal orchestrated invocation** – Long-running “intelligent process” workflows trigger the same agent runtime via a registered Temporal activity (see [305-workflows](305-workflows.md)). The activity wrapper calls into the gRPC surface, streams partial progress through Temporal heartbeats or signals, and respects workflows-level retries, timeouts, and human-in-the-loop pauses. The Temporal path exchanges the structured `AgentStepOutput` payload described in the workflows backlog to keep replay deterministic.
- Both modes share the registry, artifact cache, dependency injection, and observability stack. The gateway selects a mode based on the caller (e.g., synchronous threads vs. background workflows) and includes routing metadata so the control plane can audit usage across paths.

### 5.2 Registry Cache

- Subscribe to `AgentsService.StreamCatalog`.
- Persist node definitions + versions in an in-memory registry keyed by `(node_id, version)`.
- Maintain agent instances (tenant-scoped) with resolved parameters and routing rules.
- Cache compiled artifact metadata (`compiled_artifact_uri`, `compiled_artifact_hash`, `compiled_at`) so runtime fetches from MinIO without rehydrating state from scratch.
- Hot-reload registries on stream updates; expose metrics (`registry_nodes_total`, `registry_reload_duration_seconds`).

### 5.3 LangGraph Compilation

1. Determine agent instance via routing (see 5.3).
2. Fetch agent node definition + version.
3. Build LangChain prompts, models, memory, tools using runtime adapters:
   - Prompts: system message, fallback models, intermediate step flags.
   - Tools: instantiate factories from tool node definitions; inject dependencies.
   - Output parsers / fallback: derived from property schema.
   - Middleware: hydrate the middleware stack defined in the agent artifact, wiring built-ins (PII redaction, summarization, human-in-the-loop) and tenant-specific custom middleware as introduced in [LangChain v1](313-lanchain-v1.md).
4. Compile LangGraph graph and return streaming iterator.
5. Persist compiled artifacts to MinIO (S3-compatible) during publish; inference requests hydrate the graph by fetching `compiled_artifact_uri`/`compiled_artifact_hash` first and only fall back to on-demand compilation when the artifact is missing or stale.

### 5.4 Routing Engine

- Evaluate ordered `RoutingRule` objects with priorities.
- Conditions include user role, UI page, graph/context flags, custom metadata.
- Gateway can request a dry-run route (for routing sandbox).
- Default fallback agent defined per tenant (from Agents service).
- Populate `RouteContext` directly from the structured invocation envelope so rule evaluation sees the same inputs that n8n’s `IExecuteFunctions` exposes (role, page, selection flags, metadata).

### 5.5 Tool Runtime

- Tool factories conform to `ToolBuilder` protocol and must return LangChain `Tool`/`DynamicStructuredTool` instances enriched with the same metadata fields n8n sets inside [`getConnectedTools`](../reference-code/n8n/packages/@n8n/nodes-langchain/utils/helpers.ts#L201) (`metadata.sourceNodeName`, `metadata.isFromToolkit`). This keeps downstream executors aligned with [`ToolExecutor.node.ts`](../reference-code/n8n/packages/@n8n/nodes-langchain/nodes/ToolExecutor/ToolExecutor.node.ts#L21).

```python
class ExecutionContext(Protocol):
    def get_parameter(self, name: str, *, default: Any | None = None) -> Any: ...
    def get_input_data(self, index: int) -> Mapping[str, Any]: ...
    async def get_binary_stream(self, binary_id: str) -> AsyncIterator[bytes]: ...
    def emit_progress(self, message: str) -> None: ...

class ToolBuilder(Protocol):
    config: ToolConfig
    def build(
        self,
        context: ParsedContext,
        deps: ToolDependencies,
        exec_ctx: ExecutionContext,
    ) -> ToolConfiguration: ...  # returns tool runnable + metadata bundle
```

- `ToolConfiguration` encapsulates the runnable LangChain tool, serialized parameters, and metadata flags required by the gateway/UI to mirror [`getConnectedTools`](../reference-code/n8n/packages/@n8n/nodes-langchain/utils/helpers.ts#L201).
- Tool descriptors loaded from Agents service include node id, version, parameter map, and serialized metadata so builders can recreate the same runtime contract n8n’s [`ToolCalculator.node.ts`](../reference-code/n8n/packages/@n8n/nodes-langchain/nodes/tools/ToolCalculator/ToolCalculator.node.ts#L21) exposes.
- Toolkit support: generic executor mirroring [`ToolExecutor`](../reference-code/n8n/packages/@n8n/nodes-langchain/nodes/ToolExecutor/ToolExecutor.node.ts#L21) handles toolkit fan-out and ensures `ToolBuilder` outputs forward `tool.metadata.isFromToolkit` for routing.

#### Toolkit executor adapter

```python
async def execute_toolkit_call(request: ToolInvocation, toolkit: Toolkit) -> ToolResult:
    for tool in toolkit.get_tools():
        if tool.name == request.tool_name:
            try:
                return await execute_single_tool(tool, request.payload)
            except Exception as exc:
                raise ToolExecutionError(tool.name, str(exc)) from exc
    raise ToolExecutionError(request.tool_name, "not granted in toolkit")
```

The runtime exposes the same semantics as [`ToolExecutor`](../reference-code/n8n/packages/@n8n/nodes-langchain/nodes/ToolExecutor/ToolExecutor.node.ts#L41): single tools are invoked directly, toolkits enumerate `get_tools()`, and `ToolExecutionError` propagates back through LangGraph’s event stream so the gRPC layer can emit structured failures.

### 5.6 Streaming & Events

- gRPC `Stream` API emits:
  - `status` events (RUNNING, COMPLETED, FAILED)
  - `token` events for incremental LLM output
  - `tool_start`/`tool_end` events with payloads
  - `content` events that forward LangChain v1 `content_blocks` (reasoning traces, images, structured JSON) so the UI can render multimodal output uniformly
- HTTP SSE and WebSocket wrappers reuse same event format, matching the event cadence produced by n8n’s [`toolsAgentExecute`](../reference-code/n8n/packages/@n8n/nodes-langchain/nodes/agents/Agent/agents/ToolsAgent/V3/execute.ts#L210) so UI components can be shared.
- When invoked through Temporal, the AgentBridge activity batches streamed tokens before signaling workflows to prevent event-history blowup; default chunk interval is 100–250 ms with Continue-As-New for long conversations (see [305-workflows](305-workflows.md)).
- Every streamed message carries `StreamEventMeta` (`stream_id`, monotonic `event_index`) so downstream consumers can replay the same envelope shape that n8n’s `processEventStream` relies on.
- LangGraph events are translated to proto messages via an adapter:

```python
async def adapt_langgraph_stream(stream: AsyncIterator[StreamEvent]) -> AsyncIterator[InferenceStreamEvent]:
    async for event in stream:
        match event.event:
            case "on_threads_model_stream":
                yield InferenceStreamEvent.token(event.data.chunk.content)
            case "on_threads_model_content":
                yield InferenceStreamEvent.content(event.data.content_blocks)
            case "on_tool_start":
                yield InferenceStreamEvent.tool_start(event.data.tool_name, event.data.input)
            case "on_tool_end":
                yield InferenceStreamEvent.tool_end(event.data.tool_name, event.data.output)
            case "on_chain_error":
                yield InferenceStreamEvent.error(event.data.error)
```

- Back-pressure is enforced by awaiting the client `StreamObserver.write()` futures; slow consumers naturally stall the async generator and prevent unbounded buffering. Errors during streaming propagate through `InferenceStreamEvent.error`, after which the server sends a terminal status and closes the stream.

---

## 6. Protocol & Gateway Alignment

| Concern | Current State | Target |
|---------|---------------|--------|
| gRPC | Prototype `Generate` method lacks feature parity | Implement `Invoke` (unary) + `Stream` (server streaming) with shared proto messages; gateway switches from REST to gRPC. |
| Context envelope | JSON string parsed in service | Gateway constructs structured metadata (tenant, agent id, `ui_page`, `user_role`, selection flags, custom metadata) mirroring n8n’s `ExecutionContext`; inference consumes the typed `InvocationContext`. |
| Auth & budgeting | Managed in gateway via legacy rules | Gateway fetches agent metadata (max iterations, tool allowlist) from Agents service and enforces prior to proxying. |
| Error handling | Mixed HTTP errors / SSE strings | gRPC status codes + structured `error` events across transports. |

---

## 7. Observability & Safety

- Attach Langfuse handler to every execution (prompt, agent instance ID, tenant, tool calls).
- Emit Prometheus metrics: `agent_latency_seconds`, `tool_invocations_total`, `routing_mismatch_total`.
- Enforce limits from config (max iterations, timeout) to prevent runaway agents.
- Log tool inputs/outputs with redaction where necessary; tag with `agent_instance_id`.

---

## 8. Implementation Plan

_Status check (2025-02)_: **Registry skeleton** and **Routing engine** exist in code but still rely on refresh polling and imperative selection; **Runtime adapter**, **Streaming adapter**, and **Tool DI** remain stubs because the runtime echoes responses. The checklist below reflects the original intent and should be re-baselined once the LangGraph build pipeline lands.

1. **Registry skeleton**
   - Implement catalog client (Connect gRPC) and in-memory registries.
   - Replace static `TOOL_CONFIGS` / `AGENT_CONFIGS` with registry lookups.
2. **Routing engine**
   - Evaluate proto-driven rules, surface metrics, expose dry-run endpoint.
3. **LangGraph builder refactor**
   - Break down `build_agent_graph` into modular components reading from registry.
   - Inject dependencies via `ToolDependencies`.
   - Adopt `langchain.agents.create_agent` for graph construction, wiring middleware + context schemas and ensuring `content_blocks` propagate through streaming adapters.
4. **Tool migration**
   - Port existing graph tools to new builder interface.
   - Add toolkit executor abstraction + shared `ExecutionContext` helpers (binary payloads, progress events).
   - Capture metadata parity (`sourceNodeName`, `isFromToolkit`) across toolkit tools.
5. **gRPC parity**
   - Implement `Invoke` + `Stream` gRPC endpoints.
   - Update REST/SSE/WebSocket layers to reuse new runtime pipeline.
   - Ship LangGraph→gRPC stream adapter with structured error propagation + back-pressure tests.
6. **Gateway integration**
   - Update inference gateway to call gRPC endpoints, fetch routing data from Agents service.
7. **Observability**
   - Instrument Langfuse, Prometheus, structured logs.
8. **Cleanup**
   - Remove legacy context maps, static registries.
   - Document migration + fallback strategy.

---

## 9. Success Criteria

- [ ] Inference service boots without hardcoded agent/tool dictionaries.
- [ ] Registry hot-reload updates active agents without restarts.
- [ ] Routing decisions traceable and tenant-configurable.
- [ ] gRPC `Invoke`/`Stream` used by inference gateway (REST kept only as convenience wrapper).
- [ ] Tool factories consume shared DI dependencies; no direct environment lookups inside tools.
- [ ] Langfuse traces include `agent_instance_id` and tool events; Prometheus metrics cover latency/tool usage.
- [ ] LangGraph→gRPC adapter passes streaming soak tests with back-pressure + structured tool events.
- [ ] Documentation explains how new agent definitions propagate from Agents service to inference.

---

## 10. References

- n8n LangChain sources: `reference-code/n8n/packages/@n8n/nodes-langchain/`
  - `agents/Agent/Agent.node.ts`, `Agent/V3/AgentV3.node.ts`
  - `agents/Agent/agents/ToolsAgent/options.ts`, `.../V3/execute.ts`
  - `tools/ToolCalculator/ToolCalculator.node.ts`
  - `ToolExecutor/ToolExecutor.node.ts`
- Current inference code:
  - `services/inference/app.py`, `context.py`, `tools/*`, `agents/*`
  - `services/inference/config/routing.py`
- Agents service specification: `backlog/310-agents-v2.md`

---

**Author**: Platform Architecture  
**Last Updated**: 2025-02-01
