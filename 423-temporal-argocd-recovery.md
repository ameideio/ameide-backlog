# 423: Temporal ArgoCD recovery (dev)

> ✅ **Update:** Temporal is now deployed via the community Temporal operator (`cluster-temporal-operator`) and `TemporalCluster`/`TemporalNamespace` CRs (`sources/charts/platform/temporal-cluster`). The older Helm chart + `data-migrations-temporal` + `data-temporal-namespace-bootstrap` flow is removed; the legacy notes below remain for historical incident context.

> **Cross-References (Deployment Architecture Suite)**:
>
> | Document | Purpose |
> |----------|---------|
> | [465-applicationset-architecture.md](465-applicationset-architecture.md) | How data-temporal apps are generated by ApplicationSet |
> | [447-waves-v3-cluster-scoped-operators.md](447-waves-v3-cluster-scoped-operators.md) | RollingSync ensures migrations (265) before runtime (450/455) |
> | [464-chart-folder-alignment.md](464-chart-folder-alignment.md) | Chart at `sources/charts/third_party/temporal/` |
> | [426-keycloak-config-map.md](426-keycloak-config-map.md) | CNPG credential patterns |
>
> **Related**:
> - [420-temporal-cnpg-dev-registry-runbook.md](420-temporal-cnpg-dev-registry-runbook.md) – CNPG credentials & dev registry
> - [425-vendor-schema-ownership.md](425-vendor-schema-ownership.md) – Vendor schema ownership pattern
> - [429-devcontainer-bootstrap.md](429-devcontainer-bootstrap.md) – DevContainer bootstrap drives Argo/Temporal
> - [456-ghcr-mirror.md](456-ghcr-mirror.md) – GHCR mirroring for Temporal images

## Context

- Temporal is managed by the community operator (`cluster-temporal-operator`) and deployed per environment via the `data-temporal` app (TemporalCluster + TemporalNamespace CRs).
- Schema init/updates are performed by operator-owned setup/update Jobs as part of reconcile.
- Recovery is primarily: ensure operator CRDs + controller are healthy, then resync `{env}-data-temporal` and inspect the `TemporalCluster` status conditions and operator logs.

See also:

- `backlog/425-vendor-schema-ownership.md` – vendor‑owned schema pattern (Temporal, CNPG).
- `backlog/420-temporal-cnpg-dev-registry-runbook.md` – broader Temporal + CNPG + dev registry story.
- `backlog/429-devcontainer-bootstrap.md` – how DevContainer bootstrap drives Argo/Temporal in dev.
- `backlog/456-ghcr-mirror.md` – GHCR mirroring for Docker Hub rate limiting; Temporal images use GHCR mirrors.

## Issues encountered

1) **Schema missing → CrashLoop**
   - Temporal frontends/matching/history/worker crashed with `schema_version` relation missing.
   - Cause: Temporal schema not applied (or partially applied), so services failed their compatibility check.

2) **Namespace bootstrap failing**
   - `data-temporal-namespace-bootstrap` Job looped with connection refused to Temporal gRPC (7233).
   - Cause: Temporal services were down due to missing schema; bootstrap couldn’t reach the endpoint.

## Fixes applied (generic recipe)

Even with `data-migrations-temporal` owning the schema and automated sync/retry enabled, you can still hit the schema-missing crash on a fresh dev cluster (e.g., if migrations ran before CNPG was fully ready, or against a previous cluster). The recovery recipe is:

- **Run Temporal migrations hook** (`data-migrations-temporal` Application) to apply Temporal schema (default + visibility) via `temporal-sql-tool`. Output should show idempotent “duplicate update” warnings if the schema is already at the desired version.
- **Restart Temporal pods** (or let Argo/Deployment restart them) so they recheck the now‑initialized schema; all Temporal deployments should converge to Running.
- **Resync namespace bootstrap** (`data-temporal-namespace-bootstrap`) once Temporal is up; the Job should complete and the app should go Healthy.
- **Argo state:** `data-temporal` and `data-temporal-namespace-bootstrap` should both be Synced/Healthy with no lingering schema errors once the above is complete.

## Verification steps (dev)

- `argocd app get argocd/data-temporal` → Health: Healthy; deployments Running.
- `argocd app get argocd/data-temporal-namespace-bootstrap` → Health: Healthy; Job succeeded.
- Temporal pods in `ameide` namespace all Running; no CrashLoopBackOff.
- Database: `schema_version` exists and reflects latest Temporal version (as reported by `temporal-sql-tool`).

## Recovery Incident: 2025-12-03

### Root Cause Analysis

The Temporal migration failed due to multiple compounding issues:

1. **PostgreSQL 15+ Schema Ownership Change**: PostgreSQL 15+ no longer grants world-writable access to the `public` schema by default. The `temporal` user could not create tables in the `public` schema of its own database because CNPG creates databases with `dbuser` as the owner.

2. **Partial Migration State**: The `temporal-sql-tool` ran `setup-schema` which created tables and set `schema_version` to `0.0`, but then `update-schema` failed on an INSERT statement (`INSERT INTO namespace_metadata`) because:
   - The job has `backoffLimit: 3`, causing retries
   - First pod created tables successfully
   - Retry pods hit "duplicate key" errors on the INSERT
   - The tool ignores "table already exists" but NOT "duplicate key" errors

3. **ArgoCD Hook State**: The migration job got stuck in "Terminating" state due to finalizers, blocking subsequent syncs.

### Fixes Applied

#### 1. CNPG Database CRD with Schema Ownership (Declarative)

Modified the postgres_clusters Helm chart to support the `schemas` field in the CNPG Database CRD:

**File**: `sources/charts/foundation/operators-config/postgres_clusters/templates/databases.yaml`
```yaml
{{- if $db.schemas }}
  schemas:
{{ toYaml $db.schemas | indent 4 }}
{{- end }}
```

**File**: `sources/values/_shared/data/platform-postgres-clusters.yaml`
```yaml
databases:
  - name: temporal
    owner: temporal
    schemas:
      - name: public
        owner: temporal
  - name: temporal-visibility
    dbName: temporal_visibility
    owner: temporal_visibility
    schemas:
      - name: public
        owner: temporal_visibility
  # ... similar for all other databases
```

This ensures each database's `public` schema is owned by the database owner, fixing the PostgreSQL 15+ permission issue.

#### 2. Migration Job Idempotency (Added but Incomplete)

Added version checking to the migration job to skip if schema is already at target version:

**File**: `sources/charts/platform-layers/temporal-migrations/templates/job.yaml`
- Added `version_gte()` function to compare version strings
- Calls `temporal-sql-tool describe-schema` before running migrations
- Skips migration if current version >= target version

**File**: `sources/values/_shared/data/data-migrations-temporal.yaml`
```yaml
databases:
  default:
    targetVersion: "1.18"
  visibility:
    targetVersion: "1.9"
```

**Note**: This idempotency check helps for future runs but doesn't handle partial migration states where tables exist but version is 0.0.

#### 3. Manual Recovery of Partial State

When migration failed mid-way (tables created, version=0.0, namespace_metadata row inserted):

```bash
# Delete the conflicting row that caused duplicate key errors
kubectl exec -n ameide data-temporal-admintools-... -- \
  env PGPASSWORD=dbpassword psql \
  -h platform-postgres-clusters-rw.ameide.svc.cluster.local \
  -U temporal -d temporal \
  -c "DELETE FROM namespace_metadata WHERE partition_id = 54321;"

# Delete stuck job and clear ArgoCD operation state
kubectl delete job data-migrations-temporal-temporal-migrations -n ameide --force --grace-period=0
kubectl patch application data-migrations-temporal -n argocd --type=merge -p '{"operation": null}'

# Trigger fresh sync
kubectl annotate application data-migrations-temporal -n argocd argocd.argoproj.io/refresh=hard --overwrite
kubectl patch application data-migrations-temporal -n argocd --type=merge -p '{"operation": {"initiatedBy": {"username": "admin"}, "sync": {"prune": true}}}'
```

### Final State

After recovery:
- `temporal` database: schema version **1.18**
- `temporal_visibility` database: schema version **1.9**
- All Temporal pods: **Running**
- ArgoCD apps: `data-migrations-temporal`, `data-temporal`, `data-temporal-namespace-bootstrap` all **Synced/Healthy**

### Automation Follow-up: Guaranteed Re-runs + Setup Guardrail (2025-12-04)

To avoid manual intervention the next time the hook needs to run:

1. **Hook auto-trigger** – The `data-migrations-temporal` Helm chart now derives the `force-trigger` annotation automatically from the chart version + target schema versions. Any bump to either value causes ArgoCD to see a diff and rerun the pre-install hook; no more timestamp edits in `sources/values/_shared/data/data-migrations-temporal.yaml`.
2. **Skip duplicate `setup-schema`** – The hook template now calls `setup-schema` only if `describe-schema` fails. If the schema already exists (even if the version is stuck at `0.0`), the job jumps straight to `update-schema`, which prevents the duplicate-key crash that previously required manual clean-up.
3. **Auto-heal namespace_metadata conflicts (2025-12-12)** – The same hook now runs a guarded cleanup before `update-schema` when the default schema reports version `0.0` or missing. It issues `DELETE FROM namespace_metadata WHERE partition_id=54321` (matching the earlier manual fix) so the vendor tool doesn’t panic on the duplicate-key insert. The cleanup only runs for the default schema, keeping visibility untouched. This means fresh/partial installs self-recover without human intervention.

> **Reminder:** The namespace bootstrap job (`data-temporal-namespace-bootstrap`) still depends on Temporal’s gRPC endpoint. Run the migrations first, ensure the Temporal deployments are healthy, and only then re-sync the namespace bootstrap Application.

### Files Modified

| File | Change |
|------|--------|
| `sources/charts/foundation/operators-config/postgres_clusters/templates/databases.yaml` | Added `schemas` field support |
| `sources/charts/foundation/operators-config/postgres_clusters/values.yaml` | Added schemas to all database definitions |
| `sources/values/_shared/data/platform-postgres-clusters.yaml` | Added schemas with correct owners for all databases |
| `sources/charts/platform-layers/temporal-migrations/templates/job.yaml` | Added version checking for idempotency |
| `sources/charts/platform-layers/temporal-migrations/values.yaml` | Added `targetVersion` to database configs |
| `sources/values/_shared/data/data-migrations-temporal.yaml` | Added `targetVersion` for default (1.18) and visibility (1.9) |

### Known Limitations

1. **temporal-sql-tool limitations**: The tool's `update-schema` command doesn't treat "duplicate key" INSERT errors as ignorable (unlike "table already exists"). This can cause failures on retry if the first attempt partially succeeded.

2. **Partial state recovery**: If a migration fails mid-way, manual intervention may be needed to either:
   - Delete conflicting rows and re-run
   - Drop the database entirely and re-run (destructive)

3. **Hook triggering**: ArgoCD hooks only run when there are actual resource changes. Adding a `force-trigger` annotation to the job values can help trigger re-runs when needed.

## Guardrails / follow-ups

- Keep Temporal schema ownership in the dedicated Temporal migrations job; rerun `data-migrations-temporal` before Temporal upgrades to ensure compatibility (see `backlog/425-vendor-schema-ownership.md`).
- If Temporal pods crash with schema errors, rerun `data-migrations-temporal`, then restart pods or allow Argo to resync, then rerun namespace bootstrap as needed.
- Monitor first rollout after Temporal version bumps for schema compatibility checks; document any vendor‑specific caveats in this backlog and cross‑link from `backlog/420-temporal-cnpg-dev-registry-runbook.md` and `backlog/429-devcontainer-bootstrap.md`.
- **PostgreSQL 15+ compatibility**: Always ensure CNPG Database CRDs include `schemas` with correct ownership for the database owner.
- **Partial migration recovery**: If migration job fails with "duplicate key" errors, check the database state and delete conflicting rows before re-running.

## Recovery Incident: 2025-12-05

### Root Cause Analysis

This incident combined password mismatch with the existing partial migration state issues:

1. **Password Mismatch Between CNPG and Temporal Services**: The CNPG credential consolidation created TWO separate entries that generated DIFFERENT passwords:
   - `temporal-db-env` (used by Temporal services) - one random password
   - `temporal-db-credentials` (used by CNPG role) - different random password

   Result: Temporal services couldn't authenticate to Postgres because CNPG set the role password from `temporal-db-credentials`, but services connected using `temporal-db-env`.

2. **Migration Job Missing Tolerations**: The `temporal-migrations` Helm chart didn't support `nodeSelector`/`tolerations`, causing pods to remain Pending on dev nodes with `ameide.io/environment: dev` taint.

3. **Partial Migration State** (same as 2025-12-03): Multiple job retries caused duplicate key errors on `INSERT INTO namespace_metadata`.

### Fixes Applied

#### 1. Consolidated Temporal Credentials (commit `8d4a519`)

Removed duplicate credential entries and updated CNPG to use the same secret as Temporal services:

**File**: `sources/values/_shared/data/platform-postgres-clusters.yaml`
```yaml
# Before: CNPG and Temporal used different secrets
managed:
  roles:
    - name: temporal
      passwordSecret:
        name: temporal-db-credentials  # Different password!

# After: CNPG uses same secret as Temporal
managed:
  roles:
    - name: temporal
      passwordSecret:
        name: temporal-db-env  # Same password as Temporal services
    - name: temporal_visibility
      passwordSecret:
        name: temporal-visibility-db-env
```

Also removed duplicate entries `temporal-password` and `temporal-visibility-password` from `credentials.appUsers`.

#### 2. Updated Temporal Helm Values (commit `6adcdb6`)

Changed secret references in Temporal chart to use the consolidated `-db-env` secrets:

**File**: `sources/values/_shared/data/data-temporal.yaml`
```yaml
server:
  config:
    persistence:
      default:
        sql:
          existingSecret: temporal-db-env  # was: temporal-db-credentials
      visibility:
        sql:
          existingSecret: temporal-visibility-db-env  # was: temporal-visibility-db-credentials
```

#### 3. Added Tolerations to Migration Job (commit `2e9acb0`)

**File**: `sources/charts/platform-layers/temporal-migrations/templates/job.yaml`
```yaml
spec:
  template:
    spec:
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
```

This allows the migration job to schedule on environment-specific nodes using tolerations from `globals.yaml`.

### Recovery Steps (for partial state)

When migration failed mid-way with databases in partial state:

```bash
# 1. Drop and recreate databases cleanly
kubectl exec -n ameide-dev postgres-ameide-1 -- psql -U postgres -c "DROP DATABASE IF EXISTS temporal;"
kubectl exec -n ameide-dev postgres-ameide-1 -- psql -U postgres -c "DROP DATABASE IF EXISTS temporal_visibility;"

# 2. Delete CNPG Database CRDs to trigger recreation
kubectl delete database postgres-ameide-temporal postgres-ameide-temporal-visibility -n ameide-dev

# 3. Sync postgres-clusters to recreate databases
kubectl annotate application dev-platform-postgres-clusters -n argocd argocd.argoproj.io/refresh=hard --overwrite
kubectl patch application dev-platform-postgres-clusters -n argocd --type=merge \
  -p '{"operation": {"initiatedBy": {"username": "admin"}, "sync": {"prune": true}}}'

# 4. Wait for databases to be created, then trigger migration
kubectl delete application dev-data-migrations-temporal -n argocd --wait=false
# ApplicationSet will recreate it and trigger hooks

# 5. After migration completes, refresh Temporal app
kubectl annotate application dev-data-temporal -n argocd argocd.argoproj.io/refresh=hard --overwrite
kubectl rollout restart deployment -n ameide-dev -l app.kubernetes.io/name=temporal
```

### Environment Isolation

The credential consolidation preserves per-environment password isolation:

1. **Helm lookup is namespace-scoped**: `lookup "v1" "Secret" $namespace $secretName` only finds secrets in the target namespace
2. **Each environment gets unique passwords**: Dev/staging/prod have separate secrets generated in their respective ArgoCD syncs
3. **No cross-env secret sharing**: The consolidation only affects secrets **within** each environment

### Files Modified

| File | Change |
|------|--------|
| `sources/values/_shared/data/platform-postgres-clusters.yaml` | Removed duplicate temporal-password entries; updated CNPG managed.roles to use `-db-env` secrets |
| `sources/values/_shared/data/data-temporal.yaml` | Updated existingSecret references to use `-db-env` secrets |
| `sources/charts/platform-layers/temporal-migrations/templates/job.yaml` | Added nodeSelector and tolerations support |
| `sources/charts/platform-layers/temporal-namespace-bootstrap/templates/namespace-bootstrap-job.yaml` | Added nodeSelector and tolerations support (commit `7270a0b`) |

### Verification

```bash
# Passwords match within environment (CNPG role and Temporal service use same)
kubectl get secret temporal-db-env -n ameide-dev -o jsonpath='{.data.password}' | base64 -d

# Schema versions after successful migration
kubectl exec -n ameide-dev postgres-ameide-1 -- psql -U postgres -d temporal -c "SELECT curr_version FROM schema_version;"
# Should show: 1.18

kubectl exec -n ameide-dev postgres-ameide-1 -- psql -U postgres -d temporal_visibility -c "SELECT curr_version FROM schema_version;"
# Should show: 1.9

# Temporal pods healthy
kubectl get pods -n ameide-dev -l app.kubernetes.io/name=temporal
```

### Final State

Recovery completed successfully:
- `temporal` database: schema version **1.18**
- `temporal_visibility` database: schema version **1.9**
- All 10 Temporal pods: **Running**
- ArgoCD apps: `dev-data-migrations-temporal`, `dev-data-temporal`, `dev-data-temporal-namespace-bootstrap` all **Synced/Healthy**

## Recovery Incident: 2025-12-05 (GHCR Mirror Alignment)

### Root Cause Analysis

This incident occurred during GHCR mirror alignment (backlog/456-ghcr-mirror.md):

1. **Busybox init container missing `nc` command**: The Temporal init container was using `cgr.dev/chainguard/busybox` which doesn't include the `nc` (netcat) command needed for the wait-for-backends script.

2. **Migration job using Docker Hub image**: The `temporal-migrations` job was using `temporalio/admin-tools` from Docker Hub, which was rate-limited.

3. **Partial migration state**: Previous failed migration attempts left the databases in partial state with `schema_version` at 0.0 and duplicate key conflicts in `namespace_metadata`.

4. **Old pods stuck in Init:0/1**: Old ReplicaSets had pods stuck waiting on the old busybox image.

### Fixes Applied

#### 1. GHCR Busybox Mirror (commit in session)

Changed Temporal init container to use GHCR mirror with netcat support:

**File**: `sources/values/_shared/data/data-temporal.yaml`
```yaml
additionalInitContainers:
  - name: wait-for-backends
    # Using GHCR mirror to avoid Docker Hub rate limits
    image: ghcr.io/ameideio/mirror/busybox:1.36.1
```

#### 2. GHCR Migration Job Image (commit in session)

**File**: `sources/values/_shared/data/data-migrations-temporal.yaml`
```yaml
image:
  repository: ghcr.io/ameideio/mirror/temporalio-admin-tools
  tag: "1.29.1-tctl-1.18.4-cli-1.5.0"
imagePullSecrets:
  - ghcr-pull
```

### Recovery Steps

```bash
# 1. Delete conflicting namespace_metadata row
kubectl exec -n ameide-dev data-temporal-admintools-... -- \
  env PGPASSWORD=... psql -h platform-postgres-clusters-rw.ameide-dev.svc.cluster.local \
  -U temporal -d temporal -c "DELETE FROM namespace_metadata WHERE partition_id = 54321;"

# 2. Delete old stuck pods (Init:0/1 from old ReplicaSets)
kubectl delete pod -n ameide-dev <old-init-stuck-pods> --force --grace-period=0

# 3. Restart affected Temporal deployments
kubectl rollout restart deployment -n ameide-dev \
  data-temporal-frontend data-temporal-matching data-temporal-worker

# 4. Delete failed namespace-bootstrap job and re-sync
kubectl delete job data-temporal-namespace-bootstrap-temporal-namespace-bootstrap \
  -n ameide-dev --force --grace-period=0
kubectl annotate application dev-data-temporal-namespace-bootstrap -n argocd \
  argocd.argoproj.io/refresh=hard --overwrite
kubectl patch application dev-data-temporal-namespace-bootstrap -n argocd \
  --type merge -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"prune":true}}}'
```

### Final State

Recovery completed successfully:
- `temporal` database: schema version **1.18**
- `temporal_visibility` database: schema version **1.9**
- All 10 Temporal pods: **Running**
- ArgoCD apps: `dev-data-migrations-temporal`, `dev-data-temporal`, `dev-data-temporal-namespace-bootstrap` all **Synced/Healthy**

### Files Modified

| File | Change |
|------|--------|
| `sources/values/_shared/data/data-temporal.yaml` | Changed busybox to GHCR mirror |
| `sources/values/_shared/data/data-migrations-temporal.yaml` | Added GHCR mirror for admin-tools image |

## Recovery Incident: 2025-12-05 (workflows-runtime Namespace Mismatch)

### Root Cause Analysis

This incident affected **staging and production** environments, causing `workflows-runtime` pods to CrashLoopBackOff after Temporal was deployed:

1. **Temporal Namespace Configuration Mismatch**: The `temporal-namespace-bootstrap` job creates a namespace called `ameide`, but the `workflows-runtime` values files for staging/production were configured to connect to the `default` namespace:
   - `sources/values/env/staging/apps/workflows-runtime.yaml`: `config.temporal.namespace: default` ❌
   - `sources/values/env/production/apps/workflows-runtime.yaml`: `config.temporal.namespace: default` ❌
   - `sources/values/env/dev/apps/workflows-runtime.yaml`: `config.temporal.namespace: ameide` ✓

   Result: `workflows-runtime` crashed with error: *"Temporal namespace is missing; create it or update configuration"*

2. **Missing Tolerations**: Staging/production `workflows-runtime` values files were missing tolerations and nodeSelector for environment isolation, causing pods to either not schedule or schedule on wrong nodes.

3. **Node Label/Taint Naming Inconsistency**: Production nodes have inconsistent naming:
   - **Taint**: `ameide.io/environment: production`
   - **Label**: `ameide.io/pool: prod`

   This required different values for tolerations (value: `production`) vs nodeSelector (pool: `prod`).

### Fixes Applied

#### 1. Staging workflows-runtime (commit `a4a9b3a`)

**File**: `sources/values/env/staging/apps/workflows-runtime.yaml`
```yaml
config:
  temporal:
    namespace: ameide  # was: default

# Tolerations for staging node pool
tolerations:
  - key: "ameide.io/environment"
    value: "staging"
    effect: "NoSchedule"
nodeSelector:
  ameide.io/pool: staging
```

#### 2. Production workflows-runtime (commit `a4a9b3a`)

**File**: `sources/values/env/production/apps/workflows-runtime.yaml`
```yaml
config:
  temporal:
    namespace: ameide  # was: default

# Tolerations for production node pool
# Note: taint value is "production" but label value is "prod"
tolerations:
  - key: "ameide.io/environment"
    value: "production"
    effect: "NoSchedule"
nodeSelector:
  ameide.io/pool: prod
```

### Recovery Steps

```bash
# 1. Push the fix
git add sources/values/env/staging/apps/workflows-runtime.yaml sources/values/env/production/apps/workflows-runtime.yaml
git commit -m "fix(workflows-runtime): align Temporal namespace with dev and add tolerations"
git push

# 2. Refresh ArgoCD apps
kubectl annotate application staging-workflows-runtime production-workflows-runtime -n argocd \
  argocd.argoproj.io/refresh=hard --overwrite

# 3. Restart deployments to pick up new config
kubectl rollout restart deployment workflows-runtime -n ameide-staging
kubectl rollout restart deployment workflows-runtime -n ameide-prod
```

### Verification

```bash
# Check pods are running
kubectl get pods -n ameide-staging -l app=workflows-runtime
kubectl get pods -n ameide-prod -l app=workflows-runtime

# Verify ConfigMap has correct namespace
kubectl get configmap workflows-runtime-config -n ameide-staging -o jsonpath='{.data.TEMPORAL_NAMESPACE}'
# Should return: ameide

kubectl get configmap workflows-runtime-config -n ameide-prod -o jsonpath='{.data.TEMPORAL_NAMESPACE}'
# Should return: ameide
```

### Final State

Recovery completed successfully:
- **Staging** `workflows-runtime`: Running ✓
- **Production** `workflows-runtime`: Running ✓
- ConfigMap `TEMPORAL_NAMESPACE`: `ameide` ✓
- Pod tolerations and nodeSelector correctly configured ✓

### Files Modified

| File | Change |
|------|--------|
| `sources/values/env/staging/apps/workflows-runtime.yaml` | Changed namespace from `default` to `ameide`; added tolerations/nodeSelector |
| `sources/values/env/production/apps/workflows-runtime.yaml` | Changed namespace from `default` to `ameide`; added tolerations/nodeSelector |

### Key Takeaway: Reproducible Temporal Deployment

This fix addresses the root cause of non-reproducible Temporal deployments in staging/production. The configuration is now aligned with dev:

| Component | Creates/Uses | Namespace |
|-----------|--------------|-----------|
| `temporal-namespace-bootstrap` | Creates | `ameide` |
| `workflows-runtime` | Uses | `ameide` ✓ |

### Environment Naming Gotcha

Production has inconsistent naming between taints and labels:
- Taint key/value: `ameide.io/environment: production`
- Label key/value: `ameide.io/pool: prod`

When configuring tolerations and nodeSelector, use the correct values for each:
```yaml
tolerations:
  - key: "ameide.io/environment"
    value: "production"  # matches taint
nodeSelector:
  ameide.io/pool: prod   # matches label
```
