# Domain Strategy: Unified ameide.io Architecture

## Current Implementation Status

**Local Development**: ✅ Using `.test` domains (platform.dev.ameide.io)
**TLS Management**: ✅ cert-manager with self-signed certificates
**Gateway**: ✅ Envoy Gateway v1.5.0 supporting both .test and .io domains
**DevContainer**: ✅ Fully configured with .test domains and port mappings

**Related backlogs**: [589-rpc-transport-determinism.md](../589-rpc-transport-determinism.md), [417-envoy-route-tracking.md](../417-envoy-route-tracking.md), [454-coredns-cluster-scoped.md](../454-coredns-cluster-scoped.md)

> **Update:** Auth.js v5 naming uses `AUTH_URL`/`AUTH_SECRET`. Any `NEXTAUTH_*` references in older docs are legacy and should not be introduced in new values/config.

## Executive Summary

Implement consistent domain naming across all environments. Local development uses `.test` domains for safety, while staging/production use `.io` domains. This ensures OAuth flows work correctly in each environment.

## Domain Mapping

### Service to Domain Assignment

| Service | Domain | Purpose |
|---------|--------|---------|
| `www-ameide` | `ameide.io` | Main application |
| `www-ameide` | `www.ameide.io` | Main application (redirect to apex) |
| `www-ameide_canvas` | `platform.ameide.io` | Platform/Canvas application |
| `keycloak` | `auth.ameide.io` | Authentication/SSO |
| `envoy` | `api.ameide.io` | API Gateway (gRPC-Web) |
| `grafana` | `metrics.ameide.io` | Observability dashboard |

## Environment Strategy

### 1. Local Development (k3d)

**DNS Resolution:**
- Uses `.test` domains for local development (not production `.io` domains)
- Developers add entries to `/etc/hosts` or use dnsmasq for `*.dev.ameide.io`
- DevContainer automatically configures `/etc/hosts` entries
- No real DNS required

**Access:**
- `http://platform.dev.ameide.io:8080` (via Gateway API, k3d maps 8080→80)
- `https://platform.dev.ameide.io` (with self-signed cert, k3d maps 443→443)
- `http://auth.dev.ameide.io:8080` (Keycloak via Gateway API)
- `http://api.dev.ameide.io:8080` (gRPC-Web via Gateway API)

**SSL/TLS:**
- Self-signed certificates for local HTTPS testing
- Generated by cert-manager with self-signed issuer
- Certificate secret: `ameide-wildcard-tls`
- Browser warnings expected (add exception for local dev)

**Configuration:**
```yaml
domains:
  root: "dev.ameide.io"  # Using .test for local development
  platform: "platform.dev.ameide.io"
  auth: "auth.dev.ameide.io"
protocol: "https"  # HTTPS with self-signed cert
tls:
  enabled: true
  issuer: "self-signed"  # cert-manager ClusterIssuer
  secretName: "ameide-wildcard-tls"  # Wildcard certificate for *.dev.ameide.io
```

### 2. DevContainer (Debian on macOS Host)

**DNS Resolution:**
- Container runs Debian, host machine is macOS
- DevContainer's `postCreateCommand.sh` automatically adds `.test` domains to `/etc/hosts`
- For macOS host: run `manage-host-domains.sh setup` to configure dnsmasq for `*.dev.ameide.io`
- k3d cluster runs inside DevContainer, not on host

**Special Considerations:**
- DevContainer includes full k3d cluster (Docker-in-Docker)
- Ports 8080/8443 are mapped for Gateway access
- TLS certificates managed by cert-manager (not mkcert)

**Configuration:**
```bash
# /etc/hosts entries added by postCreateCommand.sh
127.0.0.1 dev.ameide.io www.dev.ameide.io
127.0.0.1 auth.dev.ameide.io
127.0.0.1 platform.dev.ameide.io
127.0.0.1 portal.dev.ameide.io
127.0.0.1 api.dev.ameide.io
127.0.0.1 metrics.dev.ameide.io
```

**Environment Variables:**
```yaml
# Services use .test domains for local development
- AUTH_URL=https://platform.dev.ameide.io
- AUTH_KEYCLOAK_ISSUER=http://auth.dev.ameide.io:4000/realms/ameide
```

**macOS Host Setup (Optional - for accessing from host browser):**
```bash
# Run manage-host-domains.sh to set up dnsmasq for *.dev.ameide.io
./.devcontainer/manage-host-domains.sh setup

# This configures:
# - dnsmasq to resolve *.dev.ameide.io to 127.0.0.1
# - macOS resolver to use dnsmasq for .test domains
# - No TLS certificates (handled by cert-manager in k8s)

# To check status:
./.devcontainer/manage-host-domains.sh status

# To remove:
./.devcontainer/manage-host-domains.sh cleanup
```

**Note:** The k3d cluster runs inside the DevContainer with port mappings:
- Port 8080 → 80 (HTTP)
- Port 443 → 443 (HTTPS)
These are automatically configured by `postCreateCommand.sh`

### 3. Azure Test Environment

**DNS Resolution:**
- Azure DNS Zone: `test.ameide.io`
- Real DNS with A/CNAME records
- Points to Gateway LoadBalancer IP

**Access:**
- `https://platform.test.ameide.io`
- `https://auth.test.ameide.io`
- `https://api.test.ameide.io`

**Configuration:**
```yaml
domains:
  root: "test.ameide.io"
  platform: "platform.test.ameide.io"
  auth: "auth.test.ameide.io"
protocol: "https"
tls:
  enabled: true
  issuer: "letsencrypt-staging"
```

### 4. Azure Production

**DNS Resolution:**
- Azure DNS Zone: `ameide.io`
- Production DNS records
- Points to Gateway LoadBalancer IP

**Access:**
- `https://platform.ameide.io`
- `https://auth.ameide.io`
- `https://api.ameide.io`

**Configuration:**
```yaml
domains:
  root: "ameide.io"
  platform: "platform.ameide.io"
  auth: "auth.ameide.io"
protocol: "https"
tls:
  enabled: true
  issuer: "letsencrypt-prod"
```

## Implementation Plan

### Phase 1: Local k3d Setup (✅ IMPLEMENTED)

#### 1.1 k3d cluster creation (✅ DONE)
```bash
# Actual implementation in postCreateCommand.sh
k3d cluster create ameide \
  --port "8080:80@loadbalancer" \    # Port 8080 maps to 80
  --port "8443:443@loadbalancer" \   # Port 443 maps to 443
  --k3s-arg "--disable=traefik@server:0" \
  --registry-create docker.io/ameide
```

#### 1.2 Envoy Gateway Installation (✅ DONE)

```bash
# Installed via Helmfile
helmfile -e local sync -l name=gateway

# Gateway supports both .test and .io domains
# Configuration in environments/local/platform/gateway.yaml
```

#### 1.3 Configure CoreDNS (Optional - Only for OIDC Discovery)

**When to use CoreDNS overrides:**
- ✅ OIDC discovery requires public URLs (e.g., `AUTH_KEYCLOAK_ISSUER=http://auth.ameide.io`)
- ✅ Services need to validate JWT tokens against public issuer URL
- ❌ Regular pod-to-pod communication (use `*.svc.cluster.local` instead)

**Preferred approach for pod-to-pod:**
```yaml
# Use cluster-internal DNS for pod-to-pod communication
env:
  # Public URLs for OIDC/OAuth (requires CoreDNS override)
  - name: AUTH_KEYCLOAK_ISSUER
    value: "http://auth.ameide.io:4000/realms/ameide"
  
  # Internal URLs for direct service calls (no CoreDNS needed)
  - name: KEYCLOAK_INTERNAL_URL
    value: "http://keycloak.ameide.svc.cluster.local:4000"
  # Server-only in-cluster gRPC endpoint (non-public).
  - name: AMEIDE_GRPC_BASE_URL
    value: "http://envoy-grpc:9000"
```

**If CoreDNS override is required:**
```bash
# Get the Gateway's ClusterIP
GATEWAY_IP=$(kubectl get svc -n ameide ameide-gateway -o jsonpath='{.spec.clusterIP}')

# Apply CoreDNS custom config ONLY if needed for OIDC
kubectl apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns-custom
  namespace: kube-system
data:
  ameide.override: |
    ameide.io:53 {
      hosts {
        $GATEWAY_IP ameide.io
        $GATEWAY_IP www.ameide.io
        $GATEWAY_IP platform.ameide.io
        $GATEWAY_IP auth.ameide.io
        $GATEWAY_IP api.ameide.io
      }
    }
EOF

# Restart CoreDNS
kubectl rollout restart -n kube-system deployment/coredns
```

#### 1.4 Create HTTPRoutes
```yaml
# platform-httproute.yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: platform
  namespace: ameide
spec:
  parentRefs:
    - name: ameide
  hostnames:
    - platform.ameide.io
  rules:
    - backendRefs:
        - name: www-ameide-canvas
          port: 3001
```

#### 1.5 Update Service Configurations
- Replace all `localhost:PORT` references with proper domains
- Update OAuth callback URLs
- Configure Keycloak with consistent issuer URL

### Phase 2: DevContainer Support (Week 2)

#### 2.1 Update .devcontainer/docker-compose.yml
```yaml
version: '3.8'
services:
  devcontainer:
    build: 
      context: .
      dockerfile: Dockerfile
    image: mcr.microsoft.com/devcontainers/base:debian
    extra_hosts:
      # On macOS, host-gateway maps to host.docker.internal
      # This allows Debian container to reach k3d on macOS host
      # Keep in sync with /etc/hosts (use setup-local-domains.sh)
      - "ameide.io:host-gateway"
      - "www.ameide.io:host-gateway"
      - "platform.ameide.io:host-gateway"
      - "auth.ameide.io:host-gateway"
      - "api.ameide.io:host-gateway"
    environment:
      - AUTH_URL=http://platform.ameide.io  # NextAuth v5 standard env var
      - AUTH_KEYCLOAK_ISSUER=http://auth.ameide.io:4000/realms/ameide  # Keycloak URL with port 4000
      # DevContainer-specific settings
      - DOCKER_HOST=unix:///var/run/docker-host.sock
```

#### 2.2 macOS Host Configuration
```bash
# On macOS host machine
# 1. Run the setup script (same as Phase 1)
./setup-local-domains.sh

# Or manually update /etc/hosts
sudo bash -c 'echo "127.0.0.1 ameide.io www.ameide.io platform.ameide.io auth.ameide.io api.ameide.io" >> /etc/hosts'

# 2. Ensure k3d cluster is running with exposed ports
k3d cluster create ameide \
  --port "80:80@loadbalancer" \
  --port "443:443@loadbalancer" \
  --k3s-arg "--disable=traefik@server:0" \
  --k3s-arg "--disable=servicelb@server:0"

# 3. Verify from macOS host
curl http://platform.ameide.io  # Should reach k3d ingress
```

#### 2.3 DevContainer Network Validation
```bash
# Inside DevContainer (Debian)
# Test connectivity to macOS host
ping host.docker.internal

# Test domain resolution (nslookup doesn't use /etc/hosts)
getent hosts platform.ameide.io
# Should resolve to host-gateway IP

# Alternative: use ping which respects /etc/hosts
ping -c 1 platform.ameide.io
# Should reach host.docker.internal

# Test k3d cluster access
curl http://platform.ameide.io
# Should reach the application through k3d on macOS
```

### Phase 3: Azure Test Deployment (Week 3)

#### 3.1 Create Azure DNS Zone
```bash
# Create the subzone
az network dns zone create \
  --resource-group ameide-rg \
  --name test.ameide.io

# Get the nameservers for the new zone
TEST_NS=$(az network dns zone show \
  --resource-group ameide-rg \
  --name test.ameide.io \
  --query nameServers -o tsv)

# Delegate from parent zone (ameide.io) to subzone (test.ameide.io)
for ns in $TEST_NS; do
  az network dns record-set ns add-record \
    --resource-group ameide-rg \
    --zone-name ameide.io \
    --record-set-name test \
    --nsdname $ns
done
```

#### 3.2 Configure DNS Records
```bash
# Get Gateway IP
GATEWAY_IP=$(kubectl get svc -n ameide ameide-gateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

# Create A record for apex domain with short TTL during rollout
az network dns record-set a add-record \
  --resource-group ameide-rg \
  --zone-name test.ameide.io \
  --record-set-name "@" \
  --ipv4-address $GATEWAY_IP \
  --ttl 300  # 5 minutes during rollout, increase to 3600 after stability

# Create CNAME records pointing to apex (recommended approach)
for subdomain in platform auth api www; do
  az network dns record-set cname create \
    --resource-group ameide-rg \
    --zone-name test.ameide.io \
    --name $subdomain \
    --ttl 300  # Short TTL during rollout
  
  az network dns record-set cname set-record \
    --resource-group ameide-rg \
    --zone-name test.ameide.io \
    --record-set-name $subdomain \
    --cname test.ameide.io  # All subdomains point to apex
done

# After successful rollout, increase TTLs for production
# az network dns record-set a update --ttl 3600 ...
# az network dns record-set cname update --ttl 3600 ...
```

#### 3.3 Install cert-manager (✅ DONE FOR LOCAL)
```bash
# cert-manager is installed via Helmfile
helmfile -e local sync -l name=cert-manager

# Self-signed issuer is automatically configured for local development
kubectl get clusterissuer self-signed -o yaml
```

#### 3.4 Configure Issuers (✅ SELF-SIGNED IMPLEMENTED)

##### Self-Signed Issuer (Local Development - ✅ ACTIVE)
```yaml
# Automatically created by cert-manager-config chart
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: self-signed
spec:
  selfSigned: {}

# Certificate created for *.dev.ameide.io
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ameide-wildcard
  namespace: ameide
spec:
  secretName: ameide-wildcard-tls
  dnsNames:
    - "*.dev.ameide.io"
    - "dev.ameide.io"
  issuerRef:
    name: self-signed
    kind: ClusterIssuer
```

##### Let's Encrypt Staging (Test Environments)
```yaml
# letsencrypt-staging.yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-staging
spec:
  acme:
    # Staging server for testing (higher rate limits, untrusted certs)
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    email: admin@ameide.io
    privateKeySecretRef:
      name: letsencrypt-staging
    solvers:
      # HTTP-01 challenge for domain validation
      - http01:
          ingress:
            gatewayHTTPRoute:
              parentRefs:
                - name: ameide
                  namespace: ameide
                  kind: Gateway
      # DNS-01 challenge for wildcard certificates (optional)
      - dns01:
          azureDNS:
            clientID: ${AZURE_CLIENT_ID}
            clientSecretSecretRef:
              name: azuredns-config
              key: client-secret
            subscriptionID: ${AZURE_SUBSCRIPTION_ID}
            tenantID: ${AZURE_TENANT_ID}
            resourceGroupName: ameide-rg
            hostedZoneName: test.ameide.io
```

##### Let's Encrypt Production
```yaml
# letsencrypt-prod.yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    # Production server (trusted certificates, strict rate limits)
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@ameide.io
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
      # HTTP-01 for individual domains
      - http01:
          ingress:
            gatewayHTTPRoute:
              parentRefs:
                - name: ameide
                  namespace: ameide
                  kind: Gateway
      # DNS-01 for wildcard certificates (using Workload Identity)
      - dns01:
          azureDNS:
            subscriptionID: ${AZURE_SUBSCRIPTION_ID}
            resourceGroupName: ameide-rg
            hostedZoneName: ameide.io
            # Using AKS Workload Identity (no client secret)
            managedIdentity:
              clientID: ${AZURE_MANAGED_IDENTITY_CLIENT_ID}
```

##### AKS Workload Identity Setup
```bash
# 1. Enable OIDC issuer on AKS cluster
az aks update \
  --resource-group ameide-rg \
  --name ameide-cluster \
  --enable-oidc-issuer

# Get the OIDC issuer URL
AKS_OIDC_ISSUER=$(az aks show \
  --resource-group ameide-rg \
  --name ameide-cluster \
  --query "oidcIssuerProfile.issuerUrl" -o tsv)

# 2. Create managed identity for cert-manager
az identity create \
  --name cert-manager-dns-identity \
  --resource-group ameide-rg

# Get identity details
IDENTITY_CLIENT_ID=$(az identity show \
  --name cert-manager-dns-identity \
  --resource-group ameide-rg \
  --query clientId -o tsv)

# 3. Grant DNS Zone Contributor role
IDENTITY_ID=$(az identity show --name cert-manager-dns-identity --resource-group ameide-rg --query principalId -o tsv)
DNS_ZONE_ID=$(az network dns zone show --name ameide.io --resource-group ameide-rg --query id -o tsv)
az role assignment create \
  --role "DNS Zone Contributor" \
  --assignee $IDENTITY_ID \
  --scope $DNS_ZONE_ID

# 4. Create federated credential for cert-manager service account
az identity federated-credential create \
  --name cert-manager-credential \
  --identity-name cert-manager-dns-identity \
  --resource-group ameide-rg \
  --issuer $AKS_OIDC_ISSUER \
  --subject system:serviceaccount:cert-manager:cert-manager

# 5. Annotate cert-manager service account with client ID
kubectl annotate serviceaccount cert-manager \
  -n cert-manager \
  azure.workload.identity/client-id=$IDENTITY_CLIENT_ID \
  --overwrite

# 6. Label the service account for workload identity
kubectl label serviceaccount cert-manager \
  -n cert-manager \
  azure.workload.identity/use=true \
  --overwrite
```

#### 3.5 HTTPRoute with Automatic TLS
```yaml
# platform-httproute.yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: platform
  namespace: ameide
spec:
  parentRefs:
    - name: ameide
  hostnames:
    - platform.test.ameide.io
  rules:
    - backendRefs:
        - name: www-ameide-canvas
          port: 3001
```

**Note**: TLS is handled by the Gateway with cert-manager annotations. HTTPS redirect is configured via a separate HTTPRoute.

### Phase 4: Production Deployment (Week 4)

#### 4.1 DNS Configuration Best Practices

**TTL Strategy:**
- **During Rollout**: Use 300 seconds (5 minutes) for quick changes
- **After Stability**: Increase to 3600 seconds (1 hour) for better caching
- **Emergency Changes**: Can temporarily reduce to 60 seconds if needed

**DNS Records Structure:**
```bash
# Production DNS setup
# A record for apex
ameide.io.           A      300  <GATEWAY_IP>

# CNAME records for all subdomains
www.ameide.io.       CNAME  300  ameide.io.
platform.ameide.io.  CNAME  300  ameide.io.
auth.ameide.io.      CNAME  300  ameide.io.
api.ameide.io.       CNAME  300  ameide.io.
metrics.ameide.io.   CNAME  300  ameide.io.
```

**Benefits of CNAME approach:**
- Single IP to update (apex A record only)
- Consistent routing for all subdomains
- Easier SSL certificate management
- Better for CDN/proxy integration

#### 4.2 WWW Subdomain Handling

```yaml
# HTTPRoute for apex domain
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: main-app
  namespace: ameide
spec:
  parentRefs:
    - name: ameide
  hostnames:
    - ameide.io
  rules:
    - backendRefs:
        - name: www-ameide
          port: 3000

---
# HTTPRoute for www redirect to apex
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: www-redirect
  namespace: ameide
spec:
  parentRefs:
    - name: ameide
  hostnames:
    - www.ameide.io
  rules:
    - filters:
        - type: RequestRedirect
          requestRedirect:
            hostname: ameide.io
            statusCode: 301
```

**SEO and User Experience:**
- Redirects preserve search rankings
- Users typing "www" reach the correct site
- Consistent URL structure for bookmarks
- Compatible with HSTS includeSubDomains

#### 4.3 Blue-Green Deployment
- Deploy to staging slot
- Test OAuth flows with short TTLs
- Switch DNS to new deployment (wait for TTL expiry)
- Monitor for issues
- Increase TTLs after 24 hours of stability

## Configuration Templates

### Helm Values Structure

**IMPORTANT**: Helm does NOT template values files. Use helpers in templates instead.

#### Values Files (Static Configuration Only)

```yaml
# values-base.yaml - NO TEMPLATING HERE
global:
  domain:
    root: ""  # Will be overridden by environment-specific values
    protocol: ""  # Will be overridden
    
  # Static internal URLs (these don't change)
  internal:
    auth: "http://keycloak.ameide.svc.cluster.local:4000"
    api: "http://envoy-grpc:9000"
    cache: "redis://redis-node-1.redis-headless.ameide.svc.cluster.local:6379"

# values-local.yaml
environment:
  name: "local"
  domain: "ameide.io"
  protocol: "http"

# values-test.yaml
environment:
  name: "test"
  domain: "test.ameide.io"
  protocol: "https"

# values-prod.yaml
environment:
  name: "production"
  domain: "ameide.io"
  protocol: "https"
```

#### Helm Template Helpers

```yaml
# templates/_helpers.tpl
{{- define "ameide.domain" -}}
{{- .Values.environment.domain | default "ameide.io" -}}
{{- end -}}

{{- define "ameide.protocol" -}}
{{- .Values.environment.protocol | default "http" -}}
{{- end -}}

{{- define "ameide.url" -}}
{{- $proto := include "ameide.protocol" . -}}
{{- $domain := include "ameide.domain" . -}}
{{- printf "%s://%s" $proto $domain -}}
{{- end -}}

{{- define "ameide.subdomainUrl" -}}
{{- $proto := include "ameide.protocol" . -}}
{{- $domain := include "ameide.domain" . -}}
{{- $subdomain := .subdomain -}}
{{- printf "%s://%s.%s" $proto $subdomain $domain -}}
{{- end -}}

{{- define "ameide.platformUrl" -}}
{{- include "ameide.subdomainUrl" (dict "Values" .Values "subdomain" "platform") -}}
{{- end -}}

{{- define "ameide.authUrl" -}}
{{- include "ameide.subdomainUrl" (dict "Values" .Values "subdomain" "auth") -}}
{{- end -}}

{{- define "ameide.apiUrl" -}}
{{- include "ameide.subdomainUrl" (dict "Values" .Values "subdomain" "api") -}}
{{- end -}}
```

#### Using Helpers in Templates

```yaml
# templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "ameide.fullname" . }}-config
data:
  # Use template helpers for computed values
  AUTH_URL: {{ include "ameide.platformUrl" . | quote }}
  AUTH_KEYCLOAK_ISSUER: {{ printf "%s/realms/ameide" (include "ameide.authUrl" .) | quote }}
  API_PUBLIC_URL: {{ include "ameide.apiUrl" . | quote }}
  
  # Static internal URLs from values
  API_INTERNAL_URL: {{ .Values.global.internal.api | quote }}
  REDIS_URL: {{ .Values.global.internal.cache | quote }}
```

### Service Configuration in Templates

```yaml
# templates/deployment.yaml (example for www-ameide_canvas)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "ameide.fullname" . }}-canvas
spec:
  template:
    spec:
      containers:
        - name: canvas
          env:
            # Computed URLs using helpers
            - name: AUTH_URL
              value: {{ include "ameide.platformUrl" . | quote }}
            - name: AUTH_KEYCLOAK_ISSUER
              value: {{ printf "%s/realms/ameide" (include "ameide.authUrl" .) | quote }}
            - name: NEXT_PUBLIC_API_URL
              value: {{ include "ameide.apiUrl" . | quote }}
            
            # Static internal URLs from values
            - name: API_INTERNAL_URL
              value: {{ .Values.global.internal.api | quote }}
            - name: REDIS_URL
              value: {{ printf "%s/0" .Values.global.internal.cache | quote }}
            
            # Static config from values
            - name: AUTH_KEYCLOAK_ID
              value: {{ .Values.keycloak.clientId | default "platform-app" | quote }}
            - name: AUTH_TRUST_HOST
              value: {{ .Values.auth.trustHost | default "true" | quote }}
```

### Alternative: Using ConfigMap Reference

```yaml
# templates/deployment.yaml (cleaner approach)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "ameide.fullname" . }}-canvas
spec:
  template:
    spec:
      containers:
        - name: canvas
          envFrom:
            - configMapRef:
                name: {{ include "ameide.fullname" . }}-config
          env:
            # Secrets from external secret
            - name: AUTH_KEYCLOAK_SECRET
              valueFrom:
                secretKeyRef:
                  name: {{ include "ameide.fullname" . }}-auth
                  key: client-secret
```

## Keycloak Configuration (Actual Platform Implementation)

### Overview

The platform uses Bitnami's Keycloak Helm chart with CloudNativePG for database backend.

### Base Configuration

```yaml
# charts/values/infrastructure/keycloak.yaml
auth:
  adminUser: admin
  adminPassword: changeme  # Override with secrets in production

# External PostgreSQL (CloudNativePG)
postgresql:
  enabled: false  # Use external database

externalDatabase:
  host: postgres-keycloak-rw.ameide.svc.cluster.local
  port: 5432
  user: keycloak
  password: keycloak  # Override with secrets
  database: keycloak

# Service configuration
service:
  type: ClusterIP  # Default type, overridden per environment
  ports:
    http: 4000    # Platform standard port
    https: 8443

# Environment settings
production: false  # Set to true in production
proxy: edge       # Behind Ingress/proxy

extraEnvVars:
  - name: KEYCLOAK_ENABLE_HEALTH_ENDPOINTS
    value: "true"
  - name: KEYCLOAK_ENABLE_STATISTICS
    value: "true"
  - name: KC_HOSTNAME_STRICT
    value: "false"  # Set to "true" in production for stable issuer
  - name: KC_HTTP_ENABLED
    value: "true"   # Keep true even in production (TLS terminated at Ingress)
  - name: KEYCLOAK_EXTRA_ARGS
    value: "--import-realm"

# Local development override (environments/local/infrastructure/keycloak.yaml)
service:
  type: LoadBalancer
  httpPort: 4000  # Exposed on host
```

### Realm Configuration

The platform automatically imports the `ameide` realm on startup:

```yaml
# Realm import via ConfigMap
extraVolumes:
  - name: realm-config
    configMap:
      name: keycloak-realm-config
      items:
        - key: "ameide-realm.json"
          path: "ameide-realm.json"

extraVolumeMounts:
  - name: realm-config
    mountPath: "/opt/bitnami/keycloak/data/import"
    readOnly: true

extraEnvVars:
  - name: KEYCLOAK_EXTRA_ARGS
    value: "--import-realm"
```

### Access URLs by Environment

| Environment | Admin Console | OIDC Issuer |
|------------|---------------|-------------|
| Local k3d | `http://auth.ameide.io:4000/admin` | `http://auth.ameide.io:4000/realms/ameide` |
| DevContainer | `http://auth.ameide.io:4000/admin` | `http://auth.ameide.io:4000/realms/ameide` |
| Azure Test | `https://auth.test.ameide.io/admin` | `https://auth.test.ameide.io/realms/ameide` |
| Azure Prod | `https://auth.ameide.io/admin` | `https://auth.ameide.io/realms/ameide` |

### Client Configuration

The platform creates these OAuth clients in the `ameide` realm:

1. **platform-app** - For www-ameide_canvas
   - Client ID: `platform-app`
   - Client Protocol: `openid-connect`
   - Access Type: `confidential`
   - Valid Redirect URIs:
     - `http://platform.ameide.io/*`
     - `https://platform.ameide.io/*`
   - Web Origins: `+` (same as redirect URIs)

2. **admin-cli** - For administrative tasks
   - Client ID: `admin-cli`
   - Access Type: `public`

### Health Checks

```yaml
# Keycloak health endpoints
startupProbe:
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 30
  failureThreshold: 30

livenessProbe:
  httpGet:
    path: /
    port: http

readinessProbe:
  httpGet:
    path: /realms/master
    port: http
```

### Environment Variables

```yaml
extraEnvVars:
  - name: KC_HOSTNAME
    value: "{{ .Values.keycloak.publicHostname }}"  # e.g., https://auth.test.ameide.io
  - name: KC_HOSTNAME_STRICT
    value: "true"   # Enforce stable issuer URL (critical for production)
  - name: KC_HTTP_ENABLED
    value: "true"   # Always true - TLS is handled by Ingress, not Keycloak
  - name: KC_PROXY
    value: "edge"   # Behind proxy/ingress
  - name: KC_PROXY_HEADERS
    value: "xforwarded"  # Trust X-Forwarded-* headers
  - name: KEYCLOAK_ENABLE_HEALTH_ENDPOINTS
    value: "true"
```

**Note on KC_HTTP_ENABLED:**
- Keep this `true` even in production
- TLS termination happens at the Ingress controller
- Keycloak communicates with Ingress over HTTP internally
- The public-facing URL is still HTTPS (configured via KC_HOSTNAME)

### Keycloak Ingress Configuration

```yaml
# templates/keycloak-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: keycloak
  namespace: ameide
  annotations:
    cert-manager.io/cluster-issuer: "{{ .Values.certManager.issuer }}"
    # ssl-redirect: "true"
    # force-ssl-redirect: "{{ .Values.ingress.forceSSL | default \"false\" }}"
    # Increase buffers for large headers/tokens
    # proxy-buffer-size: "128k"
    # proxy-body-size: "8m"
    # proxy-read-timeout: "180s"
    # Backend protocol
    # backend-protocol: "HTTP"
spec:
  # Using Gateway API - no ingressClassName needed
  tls:
    - hosts:
        - {{ .Values.keycloak.hostname }}
      secretName: ameide-wildcard-tls  # Use single wildcard cert
  rules:
    - host: {{ .Values.keycloak.hostname }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: keycloak
                port:
                  number: 4000
```

### Envoy Gateway Configuration

```yaml
# Gateway configuration with automatic header handling
spec:
  listeners:
    - name: https
      protocol: HTTPS
      port: 443
      # Envoy automatically handles X-Forwarded-* headers
      # Large headers support configured via BackendTrafficPolicy
```

### NextAuth Configuration for Stable URLs

```typescript
// app/(auth)/auth.ts
export const { auth, signIn, signOut, handlers } = NextAuth({
  providers: [keycloakProvider],
  
  // Trust proxy headers for correct URLs
  trustHost: true,
  
  // Ensure stable URLs
  callbacks: {
    async redirect({ url, baseUrl }) {
      // Always use the public URL (NextAuth v5 uses AUTH_URL)
      const publicUrl = process.env.AUTH_URL || baseUrl;
      if (url.startsWith("/")) return `${publicUrl}${url}`;
      return url;
    }
  }
});

// middleware.ts (if using Express)
app.set('trust proxy', 1);  // Trust first proxy
```

### Environment-Specific Values

```yaml
# values-local.yaml
keycloak:
  publicHostname: "http://auth.ameide.io:4000"
  hostname: "auth.ameide.io"
certManager:
  issuer: "self-signed"

# values-test.yaml  
keycloak:
  publicHostname: "https://auth.test.ameide.io"
  hostname: "auth.test.ameide.io"
certManager:
  issuer: "letsencrypt-staging"

# values-prod.yaml
keycloak:
  publicHostname: "https://auth.ameide.io"
  hostname: "auth.ameide.io"
certManager:
  issuer: "letsencrypt-prod"

# Note: KC_HTTP_ENABLED is always "true" - TLS termination at Ingress
```

## Let's Encrypt SSL/TLS Strategy

### Certificate Management Overview

Let's Encrypt provides free, automated SSL/TLS certificates with 90-day validity. We use cert-manager to automatically request, renew, and manage these certificates across all environments.

### Environment-Specific Approach

| Environment | Issuer | Certificate Type | Validation Method | Auto-Renewal |
|------------|--------|-----------------|-------------------|--------------|
| Local k3d | self-signed | Self-signed | N/A | Yes |
| DevContainer | self-signed | Self-signed | N/A | Yes |
| Azure Test | letsencrypt-staging | Untrusted (staging) | HTTP-01/DNS-01 | Yes |
| Azure Prod | letsencrypt-prod | Trusted | HTTP-01/DNS-01 | Yes |

### Rate Limits (Production)

Let's Encrypt enforces rate limits to ensure fair usage:

- **Certificates per Registered Domain**: 50 per week
- **Duplicate Certificate**: 5 per week
- **Failed Validation**: 5 failures per account, per hostname, per hour
- **New Orders**: 300 per 3 hours

**Mitigation Strategy:**
1. Use staging environment for all testing
2. Request wildcard certificates where possible (`*.ameide.io`)
3. Monitor certificate events and failures
4. Implement retry logic with exponential backoff

### Wildcard vs Individual Certificates

**Wildcard Certificate (`*.ameide.io`):**
- Pros: Single certificate for all subdomains, fewer rate limit concerns
- Cons: Requires DNS-01 challenge, needs Azure DNS API access
- Use for: Production environment

**Individual Certificates:**
- Pros: Simple HTTP-01 challenge, no DNS API needed
- Cons: Multiple certificates to manage, rate limit considerations
- Use for: Test environments, specific subdomains

### Wildcard Certificate Strategy

**IMPORTANT**: Wildcard `*.ameide.io` does NOT cover the apex `ameide.io`. Always include both.

```yaml
# Single wildcard certificate for all production ingresses
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ameide-wildcard
  namespace: ameide
spec:
  secretName: ameide-wildcard-tls  # All ingresses use this secret
  dnsNames:
    - "*.ameide.io"  # Covers www, platform, auth, api, metrics, and any future subdomains
    - "ameide.io"    # Must explicitly include apex domain
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  duration: 2160h    # 90 days
  renewBefore: 720h  # Renew 30 days before expiry
```

**Coverage includes:**
- `www.ameide.io` - SEO/user-friendly URL
- `platform.ameide.io` - Canvas application
- `auth.ameide.io` - Keycloak SSO
- `api.ameide.io` - gRPC-Web gateway
- `metrics.ameide.io` - Grafana dashboards
- Any future subdomains automatically covered

### Using the Wildcard Certificate

```yaml
# All production ingresses reference the same secret
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: platform
spec:
  tls:
    - hosts:
        - platform.ameide.io
      secretName: ameide-wildcard-tls  # Same secret for all
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: auth
spec:
  tls:
    - hosts:
        - auth.ameide.io
      secretName: ameide-wildcard-tls  # Same secret for all
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api
spec:
  tls:
    - hosts:
        - api.ameide.io
      secretName: ameide-wildcard-tls  # Same secret for all
```

### Staging Before Production

```bash
# 1. ALWAYS test with staging first
kubectl apply -f - <<EOF
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ameide-wildcard-staging
  namespace: ameide
spec:
  secretName: ameide-wildcard-staging-tls
  dnsNames:
    - "*.ameide.io"
    - "ameide.io"
  issuerRef:
    name: letsencrypt-staging  # STAGING FIRST
    kind: ClusterIssuer
EOF

# 2. Verify staging certificate works
kubectl describe certificate ameide-wildcard-staging -n ameide

# 3. Only then switch to production
kubectl apply -f - <<EOF
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ameide-wildcard
  namespace: ameide
spec:
  secretName: ameide-wildcard-tls
  dnsNames:
    - "*.ameide.io"
    - "ameide.io"
  issuerRef:
    name: letsencrypt-prod  # PRODUCTION
    kind: ClusterIssuer
EOF
```

### Monitoring and Alerts

```yaml
# ServiceMonitor for cert-manager metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: cert-manager
  namespace: cert-manager
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: cert-manager
  endpoints:
    - port: tcp-prometheus-servicemonitor
      path: /metrics
```

Key metrics to monitor:
- `certmanager_certificate_ready_time_seconds` - Time until certificate expires
- `certmanager_certificate_renewal_errors_total` - Failed renewal attempts
- `certmanager_http01_challenge_errors_total` - HTTP challenge failures

### Troubleshooting Common Issues

**Certificate Not Issuing:**
```bash
# Check certificate status
kubectl describe certificate platform-tls -n ameide

# Check cert-manager logs
kubectl logs -n cert-manager deploy/cert-manager

# Check challenge status
kubectl get challenges -n ameide
```

**DNS-01 Challenge Failures:**
```bash
# Verify DNS propagation
nslookup -type=txt _acme-challenge.platform.ameide.io

# Check Azure DNS permissions
kubectl logs -n cert-manager deploy/cert-manager-webhook
```

## API Gateway Configuration

### Overview

The platform uses **standalone Envoy proxy** as the API gateway, providing:
- gRPC-Web translation for browser clients
- Service mesh routing
- Load balancing
- Circuit breaking
- Observability
- Authentication/authorization
- Rate limiting

### Architecture

```
Browser → api.ameide.io → Envoy Proxy → Backend Services
         (gRPC-Web)        (port 8000)      (gRPC)
```

### Envoy Proxy Configuration (Actual Platform Implementation)

The platform uses standalone Envoy v1.27 for simplicity and consistency:
```yaml
# charts/values/platform/envoy.yaml
image:
  graph: envoyproxy/envoy
  tag: "v1.27-latest"

service:
  type: ClusterIP
  port: 8000
  targetPort: 8000

# Upstream services with actual platform ports
upstreamServices:
  - name: core-cqrs-command
    host: core-cqrs-command
    port: 6000
  - name: core-cqrs-query
    host: core-cqrs-query
    port: 6001
  - name: core-cqrs-subscription
    host: core-cqrs-subscription
    port: 6002

config:
  adminPort: 9901
  enableGrpcWeb: true
  enableCors: true
  corsOrigins:
    - "*"  # Configure appropriately for production

# Local environment override (LoadBalancer for direct access)
# environments/local/infrastructure/envoy.yaml
service:
  type: LoadBalancer
  port: 8000  # Consistent port everywhere
      filter_chains:
        - filters:
            - name: envoy.filters.network.http_connection_manager
              typed_config:
                "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
                codec_type: AUTO
                stat_prefix: ingress_http
                access_log:
                  - name: envoy.access_loggers.stdout
                    typed_config:
                      "@type": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog
                route_config:
                  name: local_route
                  virtual_hosts:
                    - name: backend
                      domains: ["*"]
                      routes:
                        - match:
                            prefix: "/ameide.core.cqrs.command.v1.CommandService/"
                          route:
                            cluster: command_service
                            timeout: 30s
                        - match:
                            prefix: "/ameide.core.cqrs.query.v1.QueryService/"
                          route:
                            cluster: query_service
                            timeout: 30s
                        - match:
                            prefix: "/ameide.core.cqrs.subscription.v1.SubscriptionService/"
                          route:
                            cluster: subscription_service
                            timeout: 0s  # No timeout for streaming
                      cors:
                        allow_origin_string_match:
                          - prefix: "http://localhost"
                          - prefix: "http://ameide.io"
                          - prefix: "https://ameide.io"
                          - prefix: "https://www.ameide.io"
                          - prefix: "http://platform.ameide.io"
                          - prefix: "https://platform.ameide.io"
                          - prefix: "https://test.ameide.io"
                          - prefix: "https://www.test.ameide.io"
                          - prefix: "https://platform.test.ameide.io"
                        allow_methods: GET, PUT, DELETE, POST, OPTIONS
                        allow_headers: keep-alive,user-agent,cache-control,content-type,content-transfer-encoding,x-accept-content-transfer-encoding,x-accept-response-streaming,x-user-agent,x-grpc-web,grpc-timeout,authorization
                        expose_headers: grpc-status,grpc-message
                        max_age: "1728000"
                http_filters:
                  - name: envoy.filters.http.grpc_web
                    typed_config:
                      "@type": type.googleapis.com/envoy.extensions.filters.http.grpc_web.v3.GrpcWeb
                  - name: envoy.filters.http.cors
                    typed_config:
                      "@type": type.googleapis.com/envoy.extensions.filters.http.cors.v3.Cors
                  - name: envoy.filters.http.router
                    typed_config:
                      "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router

  clusters:
    - name: command_service
      type: STRICT_DNS
      lb_policy: ROUND_ROBIN
      http2_protocol_options: {}
      load_assignment:
        cluster_name: command_service
        endpoints:
          - lb_endpoints:
              - endpoint:
                  address:
                    socket_address:
                      address: core-cqrs-command.ameide.svc.cluster.local
                      port_value: 6000  # Actual platform port
      health_checks:
        - timeout: 1s
          interval: 10s
          unhealthy_threshold: 2
          healthy_threshold: 2
          grpc_health_check: {}

    - name: query_service
      type: STRICT_DNS
      lb_policy: ROUND_ROBIN
      http2_protocol_options: {}
      load_assignment:
        cluster_name: query_service
        endpoints:
          - lb_endpoints:
              - endpoint:
                  address:
                    socket_address:
                      address: core-cqrs-query.ameide.svc.cluster.local
                      port_value: 6001  # Actual platform port

    - name: subscription_service
      type: STRICT_DNS
      lb_policy: ROUND_ROBIN
      http2_protocol_options: {}
      load_assignment:
        cluster_name: subscription_service
        endpoints:
          - lb_endpoints:
              - endpoint:
                  address:
                    socket_address:
                      address: core-cqrs-subscription.ameide.svc.cluster.local
                      port_value: 6002  # Actual platform port
```

#### Kubernetes Deployment
```yaml
# envoy-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: envoy
  namespace: ameide
spec:
  replicas: 2
  selector:
    matchLabels:
      app: envoy
  template:
    metadata:
      labels:
        app: envoy
    spec:
      containers:
        - name: envoy
          image: envoyproxy/envoy:v1.27-latest  # Platform uses v1.27
          ports:
            - containerPort: 8000
              name: grpc-web
            - containerPort: 9901
              name: admin
          volumeMounts:
            - name: config
              mountPath: /etc/envoy
          livenessProbe:
            httpGet:
              path: /ready
              port: 9901
            initialDelaySeconds: 10
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: 9901
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "512Mi"
              cpu: "500m"
      volumes:
        - name: config
          configMap:
            name: envoy-config
---
apiVersion: v1
kind: Service
metadata:
  name: envoy
  namespace: ameide
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: 8000
      name: grpc-web
    - port: 9901
      targetPort: 9901
      name: admin
  selector:
    app: envoy
```

#### Ingress for Grafana Metrics Dashboard
```yaml
# metrics-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: metrics
  namespace: ameide
  annotations:
    cert-manager.io/cluster-issuer: "{{ .Values.certManager.issuer }}"
    # ssl-redirect: "true"
    # Security headers for metrics dashboard
    # configuration-snippet: |
      more_set_headers "X-Frame-Options: SAMEORIGIN";
      more_set_headers "X-Content-Type-Options: nosniff";
      more_set_headers "Referrer-Policy: strict-origin-when-cross-origin";
spec:
  # Using Gateway API - no ingressClassName needed
  tls:
    - hosts:
        - metrics.ameide.io
      secretName: ameide-wildcard-tls
  rules:
    - host: metrics.ameide.io
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: grafana
                port:
                  number: 3000
---
# api-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api
  namespace: ameide
  annotations:
    cert-manager.io/cluster-issuer: "{{ .Values.certManager.issuer }}"
    # ssl-redirect: "true"
    # backend-protocol: "HTTP"  # Envoy handles gRPC-Web
    # Disable buffering for streaming
    # proxy-buffering: "off"
    # proxy-request-buffering: "off"
    # Increase body size for gRPC messages
    # proxy-body-size: "10m"
    # Extended timeouts for streaming
    # proxy-read-timeout: "3600"
    # proxy-send-timeout: "3600"
    # proxy-connect-timeout: "60"
    # Choose one: sticky hashing OR ewma latency-aware balancing
    # nginx.ingress.kubernetes.io/upstream-hash-by: "$binary_remote_addr"
    # nginx.ingress.kubernetes.io/load-balance: "ewma"
spec:
  # Using Gateway API - no ingressClassName needed
  tls:
    - hosts:
        - api.ameide.io
      secretName: ameide-wildcard-tls  # Use single wildcard cert
  rules:
    - host: api.ameide.io
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: envoy
                port:
                  number: 8000
```

### Client Configuration

#### TypeScript/JavaScript Client
```typescript
// gRPC-Web client configuration
import { GrpcWebFetchTransport } from '@protobuf-ts/grpcweb-transport';

const transport = new GrpcWebFetchTransport({
  baseUrl: process.env.NEXT_PUBLIC_API_URL || 'https://api.ameide.io',
  format: 'binary',
  credentials: 'include',
  headers: {
    'Authorization': `Bearer ${accessToken}`,
  },
});

// Use with generated service clients
const commandClient = new CommandServiceClient(transport);
const queryClient = new QueryServiceClient(transport);
```

#### Authentication Flow
```typescript
// Middleware to add auth headers
class AuthInterceptor implements RpcInterceptor {
  async interceptUnary(next: UnaryCall, method: MethodInfo, input: any, options: RpcOptions): Promise<UnaryCall> {
    const token = await getAccessToken();
    options.meta = {
      ...options.meta,
      'Authorization': `Bearer ${token}`,
    };
    return next(method, input, options);
  }
}
```

### Monitoring and Observability

#### Envoy Admin Interface
```bash
# Port-forward to access Envoy admin
kubectl port-forward -n ameide deploy/envoy 9901:9901

# Access admin interface
open http://localhost:9901

# Key endpoints:
# /stats - Metrics
# /clusters - Upstream health
# /config_dump - Current configuration
# /ready - Health check
```

#### Prometheus Metrics
```yaml
# ServiceMonitor for Envoy
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: envoy
  namespace: ameide
spec:
  selector:
    matchLabels:
      app: envoy
  endpoints:
    - port: admin
      path: /stats/prometheus
```

### Rate Limiting Configuration (Future Enhancement)

Rate limiting can be added when needed by deploying a rate limit service (e.g., Envoy's ratelimit service) and configuring the appropriate filter. For now, rate limiting is handled at the application level.

### Circuit Breaker Configuration
```yaml
# Add to cluster configuration
outlier_detection:
  consecutive_5xx: 5
  interval: 30s
  base_ejection_time: 30s
  max_ejection_percent: 50
  enforcing_consecutive_5xx: 100
  enforcing_success_rate: 100
  success_rate_minimum_hosts: 2
  success_rate_request_volume: 10
```

### Security Considerations

1. **TLS Termination**: Handled at Ingress level
2. **Authentication**: JWT validation in Envoy or backend services
3. **Authorization**: Service-level RBAC
4. **Rate Limiting**: Per-user and per-IP limits
5. **DDoS Protection**: CloudFlare or Azure Front Door in production

## Technical Considerations

### Cookie Configuration

```typescript
// Set domain for cross-subdomain cookies
const rootDomain =
  process.env.ENVIRONMENT_NAME === 'test' ? '.test.ameide.io' :
  process.env.NODE_ENV === 'production' ? '.ameide.io' : 
  undefined;  // No domain for local dev

cookies: {
  sessionToken: {
    options: {
      domain: rootDomain,
      sameSite: 'lax',
      secure: process.env.NODE_ENV !== 'development',  // HTTPS in test/prod
      httpOnly: true,
    }
  }
}
```

### CORS Configuration

```typescript
// API Gateway CORS settings - derive from environment
const PUBLIC_DOMAIN = process.env.PUBLIC_DOMAIN || 'ameide.io'; // e.g., 'test.ameide.io'
const PROTOCOL = process.env.NODE_ENV === 'development' ? 'http' : 'https';

const corsOptions = {
  origin: [
    `${PROTOCOL}://${PUBLIC_DOMAIN}`,
    `${PROTOCOL}://www.${PUBLIC_DOMAIN}`,
    `${PROTOCOL}://platform.${PUBLIC_DOMAIN}`,
    `${PROTOCOL}://auth.${PUBLIC_DOMAIN}`,
    `${PROTOCOL}://api.${PUBLIC_DOMAIN}`,
  ],
  credentials: true,
  methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],
  allowedHeaders: ['Content-Type', 'Authorization', 'x-grpc-web'],
};
```

### HTTPS Enforcement and Security Headers

```yaml
# Development/Test environment headers
metadata:
  annotations:
    # ssl-redirect: "true"
    # configuration-snippet: |
      more_set_headers "Strict-Transport-Security: max-age=86400";  # 1 day, no preload, no includeSubDomains
      more_set_headers "X-Frame-Options: SAMEORIGIN";
      more_set_headers "X-Content-Type-Options: nosniff";
      more_set_headers "Referrer-Policy: strict-origin-when-cross-origin";
      # Permissive CSP for development
      more_set_headers "Content-Security-Policy: default-src 'self' https://*.ameide.io; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline';";

# Production environment headers
metadata:
  annotations:
    # ssl-redirect: "true"
    # force-ssl-redirect: "true"
    # configuration-snippet: |
      # HSTS with includeSubDomains (covers www.ameide.io)
      # Note: Only add 'preload' after submission to hstspreload.org
      more_set_headers "Strict-Transport-Security: max-age=31536000; includeSubDomains";
      more_set_headers "X-Frame-Options: SAMEORIGIN";  # Allow same-origin embedding (Grafana, etc.)
      more_set_headers "X-Content-Type-Options: nosniff";
      more_set_headers "Referrer-Policy: strict-origin";
      # Strict CSP for production (replace with nonces/hashes)
      more_set_headers "Content-Security-Policy: default-src 'self' https://*.ameide.io; script-src 'self'; style-src 'self'; img-src 'self' data: https:; font-src 'self' data:; connect-src 'self' https://*.ameide.io; frame-ancestors 'self';";
```

**HSTS Considerations with www.ameide.io:**
- `includeSubDomains` ensures www.ameide.io is also HTTPS-only
- Prevents SSL stripping attacks on all subdomains
- Users can bookmark either www or apex safely
- Consider HSTS preload submission after production stability

### CSP Migration Strategy

```yaml
# Phase 1: Report-only mode to identify violations
Content-Security-Policy-Report-Only: default-src 'self'; script-src 'self'; report-uri /api/csp-report;

# Phase 2: Add nonces to inline scripts
Content-Security-Policy: default-src 'self'; script-src 'self' 'nonce-${NONCE}'; style-src 'self' 'nonce-${NONCE}';

# Phase 3: Production-ready CSP
Content-Security-Policy: default-src 'self' https://*.ameide.io; script-src 'self'; style-src 'self'; upgrade-insecure-requests;
```

### OAuth Redirect URIs

Keycloak client configuration must include exact redirect URIs:

**Local (k3d/DevContainer with .test domains):**
```
http://platform.dev.ameide.io:8080/*
https://platform.dev.ameide.io/*
http://dev.ameide.io:8080/*
https://dev.ameide.io/*
```

**Test Environment:**
```
https://platform.test.ameide.io/*
https://test.ameide.io/*
```

**Production:**
```
https://platform.ameide.io/*
https://ameide.io/*
```

**Note**: Using wildcard `/*` at the end allows all paths under the domain. For stricter security, use exact paths:
- `/api/auth/callback/keycloak` - OAuth callback
- `/api/auth/signin` - Sign-in page
- `/api/auth/signout` - Sign-out redirect

## Testing Strategy

### Smoke Tests

Run these commands to verify the setup is working correctly:

```bash
# 1. Verify Keycloak issuer is stable and publicly accessible
curl -s https://auth.test.ameide.io/realms/ameide/.well-known/openid-configuration | jq -r .issuer
# Expected: https://auth.test.ameide.io/realms/ameide

# 2. Verify TLS certificate SANs (Subject Alternative Names)
openssl s_client -connect platform.ameide.io:443 -servername platform.ameide.io </dev/null 2>/dev/null \
  | openssl x509 -noout -text | grep -A1 "Subject Alternative Name"
# Expected: DNS:*.ameide.io, DNS:ameide.io

# 3. Test gRPC-Web API streaming (verify buffering is disabled)
curl -i https://api.ameide.io/ameide.core.cqrs.query.v1.QueryService/Health \
  -H 'x-grpc-web: 1' \
  -H 'content-type: application/grpc-web+proto' \
  --data-binary ''
# Expected: HTTP/2 200, grpc-status header, streaming response

# 4. Verify www redirect to apex
curl -I https://www.ameide.io
# Expected: HTTP/301 or 308 redirect to https://ameide.io

# 5. Test HSTS headers
curl -I https://platform.ameide.io | grep -i strict-transport
# Expected: Strict-Transport-Security: max-age=31536000; includeSubDomains

# 6. Verify DNS resolution (all domains point to same IP)
for domain in ameide.io www.ameide.io platform.ameide.io auth.ameide.io api.ameide.io; do
  echo "$domain: $(dig +short $domain)"
done
# Expected: All domains resolve to the same ingress IP

# 7. Test OAuth discovery endpoint
curl -s https://auth.ameide.io/realms/ameide/.well-known/openid-configuration | jq '.authorization_endpoint, .token_endpoint'
# Expected: Valid OAuth endpoints

# 8. Verify CoreDNS (from within cluster)
kubectl run -it --rm debug --image=nicolaka/netshoot --restart=Never -- \
  nslookup auth.ameide.io
# Expected: Resolves to ingress controller IP (if CoreDNS override is active)
```

### Local Testing Checklist
- [ ] `/etc/hosts` file updated with .test domains (DevContainer)
- [ ] `manage-host-domains.sh` script executed for macOS host
- [ ] All services accessible via dev.ameide.io domains on ports 8080/8443
- [ ] OAuth flow works with proper redirects
- [ ] Cookies set with correct domain (`.dev.ameide.io` for local)
- [ ] API calls work across subdomains
- [ ] No CORS errors in browser console
- [ ] cert-manager certificates created (`ameide-wildcard-tls`)

### Integration Testing
- [ ] DNS records resolve correctly (use `dig` or `nslookup`)
- [ ] SSL certificates valid (check with `openssl` commands)
- [ ] Certificate SANs include wildcard and apex
- [ ] OAuth flow works end-to-end
- [ ] Session persistence across subdomains
- [ ] API authentication works with JWT tokens
- [ ] gRPC-Web streaming works without buffering

### Production Validation
- [ ] DNS propagation complete (check with `dig @8.8.8.8`)
- [ ] SSL certificates active and trusted
- [ ] OAuth callbacks working with production URLs
- [ ] Monitoring shows healthy traffic
- [ ] No mixed content warnings in browser
- [ ] HSTS headers present with `includeSubDomains`
- [ ] Performance metrics acceptable (TTFB, SSL handshake time)

## Benefits

1. **Consistency**: Same URLs across all environments
2. **Security**: Proper SSL/TLS in test/production
3. **Simplicity**: No port numbers in URLs
4. **OAuth**: Simplified callback configuration
5. **Debugging**: Easier to trace issues
6. **Production-Ready**: Local mimics production

## Risks and Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| DNS propagation delays | Users can't access | Use low TTLs during migration |
| SSL certificate issues | Browser warnings | Test with staging certs first |
| Cookie domain mismatch | Auth failures | Test cookie settings thoroughly |
| CORS errors | API calls fail | Configure CORS properly |
| Ingress misconfiguration | Services unreachable | Test with curl before DNS switch |

## Success Criteria

- [ ] All environments use ameide.io domains
- [ ] OAuth flows work identically everywhere
- [ ] No hardcoded localhost references
- [ ] SSL/TLS working in test/production
- [ ] Developers can run locally with ameide.io
- [ ] Documentation updated

Love it. Here’s a clean, incremental rollout where every step is independently testable and has a clear “done” signal. I’ve grouped by phase; each step has Goal → Do → Test → Rollback.

# Phase 0 — Repo hygiene (day 0)

### Step 0.1 — Helm helpers + env naming

**Goal:** Deterministic URLs across envs; no templating in values files.
**Do:** Add `_helpers.tpl` (your `ameide.*` helpers). Standardize to `AUTH_URL` (keep `PLATFORM_URL` as fallback).
**Test:**

```bash
helm template charts/ . -f values-local.yaml | grep -E 'https?://(auth|platform|api)\.'
# should show fully-rendered URLs
```

**Rollback:** Revert helpers; charts still work with literal values.

---

# Phase 1 — Local k3d (week 1)

### Step 1.1 — Cluster + Envoy Gateway (✅ IMPLEMENTED)

**Goal:** Reach a Gateway from `platform.dev.ameide.io`.
**Status:** ✅ Complete - k3d cluster with Envoy Gateway v1.5.0
**Test:**

```bash
kubectl get gateway -n ameide
curl -I http://platform.dev.ameide.io:8080  # Via Gateway on port 8080
```

### Step 1.2 — Local domains (✅ IMPLEMENTED)

**Goal:** Hostname resolution for .test domains.
**Status:** ✅ Complete - DevContainer uses /etc/hosts, macOS uses dnsmasq
**Test:**

```bash
# In DevContainer
cat /etc/hosts | grep dev.ameide.io
# On macOS (if configured)
./.devcontainer/manage-host-domains.sh status
```

### Step 1.3 — Platform Ingress (HTTP) (✅ IMPLEMENTED)

**Goal:** Serve applications at `http://platform.dev.ameide.io:8080`.
**Do:** Apply `platform-ingress.yaml`; deploy `www-ameide-canvas` Service.
**Test:**

```bash
curl -I http://platform.ameide.io
```

**Rollback:** `kubectl delete ingress platform -n ameide`.

### Step 1.4 — Keycloak local (✅ IMPLEMENTED)

**Goal:** Keycloak reachable at `auth.dev.ameide.io`.
**Status:** ✅ Complete - Keycloak deployed with stable issuer
**Configuration:**
* Uses both `.test` for local and `.io` for consistency
* `KC_HOSTNAME_STRICT=false` for development flexibility
* `KC_HTTP_ENABLED=true`

**Test:**
```bash
curl -s http://auth.dev.ameide.io:4000/realms/ameide/.well-known/openid-configuration | jq -r .issuer
# Returns appropriate issuer based on access
```

### Step 1.5 — Envoy (gRPC-Web) + API (✅ IMPLEMENTED)

**Goal:** gRPC-Web reachable at `http://api.dev.ameide.io:8080`.
**Status:** ✅ Complete - Envoy proxy deployed on port 8000
**Test:**

```bash
curl -i http://api.dev.ameide.io:8080/ameide.core.cqrs.query.v1.QueryService/Health \
  -H 'x-grpc-web: 1' -H 'content-type: application/grpc-web+proto' --data-binary ''
```

### Step 1.6 — Local OAuth round-trip (✅ IMPLEMENTED)

**Goal:** Full login flow using .test domains.
**Status:** ✅ Complete - OAuth working with .test domains
**Configuration:**
* `AUTH_URL=https://platform.dev.ameide.io`
* `AUTH_KEYCLOAK_ISSUER=http://auth.dev.ameide.io:4000/realms/ameide`

### Step 1.7 — Local TLS (✅ IMPLEMENTED)

**Goal:** HTTPS with self-signed certificates.
**Status:** ✅ Complete - cert-manager with self-signed ClusterIssuer
**Test:**

```bash
curl -I https://platform.dev.ameide.io -k   # self-signed cert
kubectl get certificate -n ameide ameide-wildcard
```

---

# Phase 2 — DevContainer networking (week 2)

### Step 2.1 — Host gateway mapping

**Goal:** Container reaches host k3d via host-gateway.
**Do:** Add `extra_hosts` and envs (you updated already).
**Test (inside container):**

```bash
getent hosts platform.ameide.io
curl http://platform.ameide.io
```

**Rollback:** Remove `extra_hosts`.

### Step 2.2 — End-to-end auth from container

**Goal:** Login works from inside DevContainer UI/browser.
**Do:** Ensure the same envs; cookies not scoped to domain in dev (no `domain=`).
**Test:** Sign in and hit API from app running in container; no CORS errors.
**Rollback:** Revert envs.

---

# Phase 3 — Azure TEST (week 3)

### Step 3.1 — DNS subzone + delegation

**Goal:** Public resolution for `*.test.ameide.io`.
**Do:** Create zone, add NS delegation in parent.
**Test:**

```bash
dig NS test.ameide.io +short @8.8.8.8
```

**Rollback:** Remove NS at parent; delete subzone.

### Step 3.2 — Ingress + cert-manager (staging)

**Goal:** Valid (staging) cert issued via HTTP-01.
**Do:** Install cert-manager v1.15.x; create a dummy Ingress with `cert-manager.io/cluster-issuer: letsencrypt-staging`.
**Test:**

```bash
kubectl describe certificate dummy -n ameide | grep -i "Ready"
```

**Rollback:** Delete Certificate/Ingress.

### Step 3.3 — Keycloak on TEST

**Goal:** `https://auth.test.ameide.io` with strict hostname.
**Do:** Deploy with `KC_HOSTNAME=https://auth.test.ameide.io`, `KC_HTTP_ENABLED=true`; Ingress TLS (staging).
**Test:**

```bash
curl -s https://auth.test.ameide.io/realms/ameide/.well-known/openid-configuration | jq -r .issuer
# expect: https://auth.test.ameide.io/realms/ameide
```

**Rollback:** Scale down; remove ingress.

### Step 3.4 — Platform on TEST

**Goal:** `https://platform.test.ameide.io` login works.
**Do:** Set `AUTH_URL=https://platform.test.ameide.io`. Update Keycloak client redirects to TEST URLs.
**Test:** Browser login success; session cookie scoped to `.test.ameide.io`.
**Rollback:** Remove TEST redirect URIs; redeploy app with previous envs.

### Step 3.5 — Envoy/API on TEST

**Goal:** `https://api.test.ameide.io` streams (buffering off), CORS OK.
**Do:** Deploy Envoy + API Ingress (staging cert). Add apex + platform origins in CORS.
**Test:**

```bash
curl -i https://api.test.ameide.io/ameide.core.cqrs.query.v1.QueryService/Health \
  -H 'x-grpc-web: 1' -H 'content-type: application/grpc-web+proto' --data-binary ''
```

**Rollback:** Delete API Ingress/Service.

### Step 3.6 — Grafana on TEST (metrics. subdomain)

**Goal:** `https://metrics.test.ameide.io` with security headers.
**Do:** Add Ingress + staging TLS + headers (no `DENY` if you embed).
**Test:**

```bash
curl -sI https://metrics.test.ameide.io | grep -Ei 'strict-transport|content-security-policy'
```

**Rollback:** Delete Ingress.

---

# Phase 4 — Prod hardening (week 4)

### Step 4.1 — DNS-01 + wildcard (dry-run)

**Goal:** Prove wildcard issuance path before prod.
**Do:** Configure Workload Identity + `letsencrypt-staging` DNS-01 for `*.ameide.io` in a **temporary** Certificate (different secret).
**Test:**

```bash
kubectl describe certificate ameide-wildcard-staging -n ameide | grep Ready
```

**Rollback:** Delete temp Certificate; no prod impact.

### Step 4.2 — Prod wildcard certificate

**Goal:** Real `ameide-wildcard-tls` in prod.
**Do:** Switch issuer to `letsencrypt-prod` DNS-01; SANs: `*.ameide.io` and `ameide.io`.
**Test:**

```bash
openssl s_client -connect platform.ameide.io:443 -servername platform.ameide.io </dev/null 2>/dev/null \
 | openssl x509 -noout -text | grep -A1 "Subject Alternative Name"
```

**Rollback:** Point Ingress back to per-host secrets (if any), or pause cert-manager.

### Step 4.3 — Prod Gateway + security headers

**Goal:** Uniform HTTPS + sane headers.
**Do:** Apply prod annotations (HSTS 1y with `includeSubDomains`, no `preload` yet).
**Test:**

```bash
curl -sI https://ameide.io | grep -i strict-transport
```

**Rollback:** Reapply previous annotations.

### Step 4.4 — Prod DNS (short TTL rollout)

**Goal:** All prod hostnames resolve to Ingress.
**Do:** Apex A → Ingress IP; subdomains CNAME → apex; TTL 300.
**Test:**

```bash
for d in ameide.io www.ameide.io platform.ameide.io auth.ameide.io api.ameide.io; do
  echo "$d: $(dig +short $d @8.8.8.8)"
done
```

**Rollback:** Restore previous IPs/records.

### Step 4.5 — Prod OAuth round-trip

**Goal:** Login works at scale; sessions valid across subdomains.
**Do:** Set `AUTH_URL=https://platform.ameide.io`; cookie domain `.ameide.io`.
**Test:** Browser login; cross-subdomain API call succeeds; no mixed content/CORS.
**Rollback:** Flip Ingress backends to old revision (blue/green).

---

# Phase 5 — Nice-to-haves (post-cutover)

### Step 5.1 — www redirect

**Goal:** Consistent canonical URLs.
**Do:** Add redirect filter in HTTPRoute for www to apex.
**Test:** `curl -I https://www.ameide.io` → 301/308 to apex.
**Rollback:** Remove annotation.

### Step 5.2 — Rate limiting (only when ready)

**Goal:** Add protection without breaking traffic.
**Do:** Deploy ratelimit service, add Envoy cluster, then enable filter with low enforcement.
**Test:** Synthetic burst → 429 surfaced, normal traffic OK.
**Rollback:** Remove filter (no restart needed for other paths).

---

## Lightweight automation (optional but helpful)

* Makefile targets: `make k3d-up`, `make local-tls`, `make smoke-local`, `make smoke-test`, `make smoke-prod`.
* CI job runs `helm template` and a k3d smoke test (curl + grpc-web check).

If you want, I can package the above into a checklist + `make smoke-*` scripts so every step has a one-liner verifier.

## Dependencies

- Azure DNS zone ownership
- SSL certificates (Let's Encrypt)
- Gateway controller (Envoy Gateway)
- cert-manager for automatic SSL
- DNS update permissions

## Related Documents

- [104-keycloak-config-v2.md](./104-keycloak-config-v2.md) - OAuth configuration
- [100-port-numbering-forward.md](./100-port-numbering-forward.md) - Port assignments
- [Infrastructure README](../infra/kubernetes/README.md) - K8s setup
